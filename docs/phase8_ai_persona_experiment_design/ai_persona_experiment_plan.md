# Phase 8: AI 페르소나 실험 계획

## 1. 개요

본 문서는 강화학습(RL) 에이전트의 Policy를 '정답 찾기'가 아닌 '개성(Personality) 창조'의 관점에서 접근하여, 다양한 목적을 가진 AI 페르소나를 설계하고 실험하는 계획을 정의한다. 이는 `Phase 7`에서 제시된 'AI 페르소나' 개념을 구체화하고, 게임 QA 및 밸런스 분석에 있어 다각적인 시뮬레이션 데이터를 확보하기 위함이다.

## 2. RL Policy: '찾는 것'이 아닌 '만들어내는 것'

RL 에이전트의 Policy는 단순히 게임에서 승리하는 '정답'을 찾는 것을 넘어, 특정 목적과 제약을 가진 '개성 있는' 플레이 스타일을 만들어내는 도구로 활용될 수 있다. 이를 통해 인간 플레이어의 다양한 행동 양상을 시뮬레이션하고, 게임 디자인의 잠재적 문제점을 발견한다.

## 3. 실험 계획

### 3.1. 실험 계획 1: '프로게이머' 페르소나 만들기

- **가설:** 보상 함수를 오직 '승리'와 관련된 지표(예: 입힌 데미지)에만 집중시키고 완벽한 컨트롤을 허용하면, 인간의 상식을 뛰어넘는 최적의 콤보와 전략을 찾아낼 것이다.
- **목표:** 게임의 이론적 성능 한계와 잠재적인 '버그성 플레이' 또는 '인간이 쓰기엔 너무 어려운 콤보'를 탐색한다.
- **실행 방안 (구현_알고리즘_명세서.md 기반):**
  - **Reward Hacking:** 통합 보상 함수에서 `Immersion`, `Responsiveness` 등의 가중치는 0으로 설정하고, 오직 `Balance` (승패) 관련 보상만 극대화하여 에이전트를 학습시킨다.
  - **Human Error Layer:** 실수 주입 확률을 0으로 설정하여, AI가 1프레임의 오차도 없는 완벽한 컨트롤을 하도록 만든다.
- **예상 결과:** 이 AI가 찾아낸 패턴은 '버그성 플레이'나 '인간이 쓰기엔 너무 어려운 콤보'일 수 있다. 이는 그 자체로 밸런스 QA에 가장 중요한 데이터가 된다.

### 3.2. 실험 계획 2: '초보자' 페르소나 만들기

- **가설:** AI에게 의도적으로 제약을 가하고 인간적인 실수를 주입하면, 실제 초보 유저들이 게임에서 어떤 어려움을 겪을지 시뮬레이션할 수 있다.
- **목표:** 초보 유저들이 게임에서 겪을 수 있는 '조작의 어려움', '학습 곡선' 관련 문제점을 발견한다.
- **실행 방안 (구현_알고리즘_명세서.md 기반):**
  - **Action Masking:** 커맨드가 복잡한 필살기나 콤보 시동기에 해당하는 `action`을 의도적으로 사용하지 못하게 막는다.
  - **Human Error Layer:** `mistake_probability` (입력 오타)와 `drop_probability` (입력 누락) 확률을 15~20% 수준으로 높게 설정한다.
- **예상 결과:** 이 AI가 특정 기본기만 반복해서 사용하거나 어이없는 실수를 연발한다면, 이는 초보자들이 '뭘 해야 할지 모르는' 구간이나 '조작이 너무 어려운' 구간이 존재한다는 증거가 된다.

### 3.3. 실험 계획 3: '트롤' 페르소나 만들기

- **가설:** 승리가 아닌, '상대방을 괴롭히는 것'을 목적으로 학습시키면, 유저들에게 불쾌한 경험을 주는 플레이 패턴을 미리 찾아낼 수 있다.
- **목표:** 유저들이 악용할 수 있는 어뷰징 전략이나, 게임 플레이 경험을 저해하는 패턴을 사전에 탐지한다.
- **실행 방안 (구현_알고리즘_명세서.md 기반):**
  - **Custom Reward:** 승패 보상을 0으로 설정한다. 대신, '상대방과 멀리 떨어져 있기', '상대방이 동일한 방어 행동을 계속하도록 유도하기' 등에 양의 보상을 준다.
- **예상 결과:** 이 AI가 발견한 '니가 와라' 플레이나 '특정 패턴만 반복하는 플레이'는 유저들이 악용할 수 있는 어뷰징 전략을 미리 알려주는 중요한 지표가 된다.
