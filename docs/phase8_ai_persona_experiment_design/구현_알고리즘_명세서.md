# Phase 8: AI 페르소나 실험 구현 알고리즘 명세서

## 1. 개요
본 문서는 Phase 8에서 정의된 AI 페르소나(프로게이머, 초보자, 트롤 등)를 구현하기 위한 구체적인 알고리즘 및 로직을 명세합니다. `Phase 7`에서 구축된 모듈들을 활용하고 확장하여 각 페르소나의 독특한 플레이 스타일을 강화학습 에이전트에 주입하는 방법을 다룹니다.

## 2. AI 페르소나별 구현 알고리즘

### 2.1. '프로게이머' 페르소나 구현 알고리즘
*   **목표:** 게임의 이론적 성능 한계 및 잠재적 '버그성 플레이' 탐색.
*   **알고리즘:**
    *   **보상 함수 (Custom Reward):**
        *   `Phase 7`의 통합 보상 함수(`Reward = α * StabilityScore + β * BalanceScore + γ * ResponsivenessScore + δ * ImmersionScore`)를 기반으로, `StabilityScore`, `ResponsivenessScore`, `ImmersionScore`에 대한 가중치(`α`, `γ`, `δ`)를 0에 가깝게 설정합니다.
        *   `BalanceScore` (승패, 입힌 데미지, 상대에게 입힌 경직 시간 등) 관련 지표의 가중치(`β`)를 극대화합니다.
        *   예시: `Reward = 1.0 * TotalDamageDealt - 0.5 * TotalDamageTaken + 10.0 * WinBonus`
    *   **Human Error Layer (`src/simulation/human_error_layer.py` 활용):**
        *   `reaction_time_mean`, `reaction_time_std`, `mistake_probability`, `drop_probability`를 모두 0으로 설정하여 AI가 1프레임의 오차도 없는 완벽한 컨트롤을 하도록 합니다.
    *   **학습 환경:** `Stable-Baselines3`의 `PPO` 알고리즘을 사용하여 학습하며, `training_params` (예: `learning_rate`, `n_steps`, `gamma`)는 최적의 성능을 위한 값으로 설정합니다.

### 2.2. '초보자' 페르소나 구현 알고리즘
*   **목표:** 실제 초보 유저들이 게임에서 겪을 수 있는 '조작의 어려움', '학습 곡선' 관련 문제점 발견.
*   **알고리즘:**
    *   **보상 함수 (Custom Reward):**
        *   `프로게이머` 페르소나와 유사하게 승리 관련 보상을 주되, `ImmersionScore` (예: 콤보 성공 시 보너스)의 가중치를 낮게 설정하여 복잡한 콤보 학습에 대한 동기를 줄입니다.
    *   **Human Error Layer (`src/simulation/human_error_layer.py` 활용):**
        *   `mistake_probability` (입력 오타)와 `drop_probability` (입력 누락) 확률을 15% ~ 20% 수준으로 높게 설정합니다.
        *   `reaction_time_mean`을 실제 인간의 반응 시간(예: 100ms ~ 200ms, 즉 6~12프레임)에 해당하는 값으로 설정합니다.
    *   **Action Masking (환경 확장):**
        *   게임 환경(`FightingGameEnv`) 내에서 `action_space`를 정의할 때, 커맨드가 복잡한 필살기나 콤보 시동기에 해당하는 `action`을 `masking` 처리하여 에이전트가 해당 액션을 선택할 수 없도록 합니다.
        *   예시: `env.action_space.mask = [True, True, False, True, ...]` (False인 액션은 선택 불가)
    *   **학습 환경:** `Stable-Baselines3`의 `PPO` 알고리즘을 사용하여 학습하며, `training_params`는 `프로게이머`보다 낮은 `learning_rate` 또는 적은 `n_steps`로 설정하여 학습 속도를 늦춥니다.

### 2.3. '트롤' 페르소나 구현 알고리즘
*   **목표:** 유저들에게 불쾌한 경험을 주는 플레이 패턴이나 어뷰징 전략 사전 탐지.
*   **알고리즘:**
    *   **보상 함수 (Custom Reward):**
        *   `Phase 7`의 통합 보상 함수에서 승패 관련 보상(`BalanceScore` 가중치)을 0으로 설정합니다.
        *   대신, '상대방과 멀리 떨어져 있기', '상대방이 동일한 방어 행동을 계속하도록 유도하기', '맵 구석에 숨어있기' 등 비승리 목적의 행동에 양의 보상을 부여합니다.
        *   예시: `Reward = 1.0 * DistanceToOpponent + 0.5 * OpponentRepeatedDefenseActionCount - 0.1 * DamageDealt`
    *   **Human Error Layer:** `프로게이머`와 유사하게 낮은 에러율을 유지하여 의도적인 트롤링을 수행하도록 합니다.
    *   **Curiosity-Driven Exploration (`Stable-Baselines3-Contrib`의 `ICM` 활용):**
        *   `ICM` 모듈을 활성화하여 에이전트가 기존에 시도하지 않았던 새로운 행동이나 패턴을 탐색하도록 장려합니다. 이는 예상치 못한 트롤링 패턴을 발견하는 데 도움이 될 수 있습니다.
    *   **학습 환경:** `Stable-Baselines3`의 `PPO` 알고리즘을 사용하여 학습하며, `training_params`는 탐색을 장려하는 방향으로 설정합니다.

## 3. Multi-Persona Experiment Orchestrator 알고리즘
*   **역할:** 각 페르소나 AI 모델을 로드하고, `Simulation Arena`를 초기화하며, 대전 시뮬레이션을 실행하고 로그를 수집하는 과정을 자동화합니다.
*   **알고리즘:**
    1.  **페르소나 모델 로드:** `ai_personas.py`에 정의된 각 페르소나에 대해 학습된 `Stable-Baselines3` 모델(`.zip` 파일)을 로드합니다.
    2.  **대전 쌍 생성:** 실험할 페르소나 목록에서 모든 가능한 대전 쌍을 생성합니다 (예: A vs B, A vs C, B vs C).
    3.  **시뮬레이션 루프:**
        *   각 대전 쌍에 대해 `N`회 반복 시뮬레이션을 수행합니다.
        *   **환경 초기화:** `Simulation Arena`를 초기화하고, 현재 대전 중인 페르소나의 `error_tolerance`에 따라 `Human Error Layer` 및 `Network Simulator`의 파라미터를 동적으로 설정합니다.
        *   **에이전트 연결:** 로드된 페르소나 모델을 `Simulation Arena`의 플레이어 컨트롤러에 연결합니다.
        *   **로그 수집 시작:** `LogCollector.start_session()`을 호출하여 새로운 로깅 세션을 시작합니다.
        *   **게임 실행:** `Simulation Arena`를 실행하고, 각 프레임마다 에이전트의 `action`을 받아 `Human Error Layer`와 `Network Simulator`를 거쳐 게임에 전달합니다.
        *   **이벤트 로깅:** 게임 상태, 에이전트 행동, 에러 주입, 네트워크 조건 등 모든 관련 이벤트를 `LogCollector`를 통해 기록합니다.
        *   **로그 수집 종료:** 대전 종료 시 `LogCollector.end_session()`을 호출하고 리플레이 데이터를 저장합니다.
        *   **메트릭 추출:** `MetricExtractor`를 사용하여 해당 세션의 로그 파일에서 QA 및 Immersion 메트릭을 추출하고 `DBManager`에 저장합니다.
    4.  **결과 집계:** `DBManager`에서 각 대전 쌍 및 페르소나별로 저장된 메트릭을 쿼리하여 집계하고 비교 분석을 위한 데이터를 준비합니다.

## 4. Persona Analysis & Comparison Module 알고리즘
*   **역할:** `DBManager`에 저장된 페르소나별 메트릭을 기반으로 심층적인 분석 및 비교를 수행합니다.
*   **알고리즘:**
    1.  **데이터 쿼리:** `DBManager`에서 특정 페르소나 또는 대전 쌍과 관련된 모든 세션의 QA 및 Immersion 메트릭을 쿼리합니다.
    2.  **메트릭 집계:** 쿼리된 메트릭 데이터에 대해 평균, 중앙값, 표준편차, 최소/최대값 등 통계적 분석을 수행합니다.
    3.  **비교 분석:**
        *   **승률 분석:** 각 페르소나의 승률을 계산하고, 특정 페르소나가 다른 페르소나에 비해 통계적으로 유의미하게 높은 승률을 보이는지 분석합니다.
        *   **행동 패턴 분석:** `ACTION_EVENT` 로그를 분석하여 각 페르소나의 특정 액션(예: 잽, 훅, 방어) 사용 빈도, 콤보 성공률, 이동 패턴 등을 비교합니다.
        *   **오류/지연 영향 분석:** `HUMAN_ERROR_INJECTED_EVENT` 및 `NETWORK_CONDITION_EVENT` 로그를 분석하여 각 페르소나가 오류 및 네트워크 지연에 어떻게 반응하는지, 그리고 이것이 성능에 미치는 영향을 분석합니다.
        *   **Immersion 메트릭 비교:** `TensionIndex`, `ComboRhythmScore`, `CounterPlayScore` 등을 비교하여 각 페르소나가 생성하는 플레이 경험의 질적 차이를 분석합니다.
    4.  **이상 패턴 탐지:**
        *   `프로게이머 AI`의 경우, 비정상적으로 높은 데미지 효율을 보이는 콤보나 특정 상황에서의 반복적인 '꼼수' 패턴을 탐지합니다.
        *   `초보자 AI`의 경우, 특정 구간에서 높은 실수율이나 진행 불능 상태에 빠지는 패턴을 탐지하여 난이도 디자인 개선점을 찾습니다.
        *   `트롤 AI`의 경우, 상대방의 플레이를 심각하게 저해하는 비승리 목적의 행동 패턴을 탐지합니다.
    5.  **결과 저장:** 분석 결과를 `DBManager`에 저장하거나, `Report Generator`가 활용할 수 있는 형태로 준비합니다.
