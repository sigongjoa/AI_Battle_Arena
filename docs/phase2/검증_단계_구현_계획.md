# AI 파이터 강화학습 프로젝트: 검증 단계 구현 계획

본 문서는 AI 파이터 강화학습 프로젝트에서 에이전트의 학습 진행 상황을 모니터링하고 최종 성능을 평가하기 위한 검증 단계 구현 계획을 상세히 설명합니다. 학습 중 검증과 학습 후 평가 두 가지 주요 접근 방식을 다룹니다.

## 1. 목적

*   **학습 진행 상황 모니터링**: 에이전트가 학습 과정에서 성능이 향상되고 있는지 실시간에 가깝게 확인합니다.
*   **모델 선택 및 하이퍼파라미터 튜닝**: 최적의 성능을 보이는 모델 체크포인트를 식별하고, 하이퍼파라미터 튜닝의 효과를 검증합니다.
*   **최종 성능 평가**: 학습이 완료된 에이전트의 실제 게임 환경에서의 성능을 객관적인 지표로 측정합니다.
*   **재현성 확보**: 동일한 조건에서 학습 및 평가를 반복하여 결과의 신뢰성을 높입니다.

## 2. 학습 중 검증 (In-Training Validation) 구현

학습이 진행되는 동안 주기적으로 에이전트의 성능을 평가하여 학습 곡선을 추적하고 조기 종료(Early Stopping) 등의 결정을 내릴 수 있도록 합니다.

### 2.1. Stable-Baselines3 `EvalCallback` 활용

Stable-Baselines3는 학습 중 검증을 위한 `EvalCallback`을 제공합니다. 이를 `model.learn()` 메서드에 등록하여 사용합니다.

**동작 방식**:
*   `EvalCallback`은 지정된 `eval_freq` (평가 주기)마다 학습 환경과 별개인 `eval_env`에서 에이전트를 실행합니다.
*   `n_eval_episodes` (평가 에피소드 수)만큼 에피소드를 플레이하고, 각 에피소드의 보상 및 기타 지표를 수집합니다.
*   수집된 지표들의 평균을 계산하여 TensorBoard에 자동으로 로깅합니다.
*   `best_model_save_path`를 지정하면, 가장 높은 평균 보상을 기록한 모델을 자동으로 저장합니다.

**주요 설정**:
*   `eval_env`: 검증에 사용할 환경 인스턴스. 학습 환경과 동일한 `FightingEnv`를 사용하되, 별도의 시드(seed)를 부여하여 독립성을 유지합니다.
*   `eval_freq`: 몇 스텝마다 평가를 수행할지 지정합니다. (예: `10000` 스텝마다 평가)
*   `n_eval_episodes`: 한 번 평가를 수행할 때 플레이할 에피소드 수. (예: `10` 에피소드)
*   `log_path`: TensorBoard 로그를 저장할 경로. 학습 로그와 분리하거나 동일 경로 내 하위 폴더로 관리할 수 있습니다.
*   `best_model_save_path`: 최고 성능 모델을 저장할 경로.
*   `deterministic`: 평가 시 에이전트의 행동을 결정론적으로(deterministic) 할지 여부. 일반적으로 `True`로 설정하여 일관된 성능을 측정합니다.

**TensorBoard 로깅**: `EvalCallback`은 `eval/mean_reward`, `eval/mean_ep_length` 등의 지표를 TensorBoard에 자동으로 기록합니다. `CustomTensorboardCallback`과 함께 사용하여 `eval/win_rate`와 같은 커스텀 지표도 추가할 수 있습니다.

## 3. 학습 후 평가 (Post-Training Evaluation) 구현

학습이 완료된 후, 특정 체크포인트 모델을 로드하여 에이전트의 최종 성능을 정밀하게 측정하고 보고서를 생성합니다.

### 3.1. 별도 평가 스크립트 (`evaluate_agent.py`) 생성

`train_rl_agent.py`와는 별개로 `evaluate_agent.py`라는 스크립트를 생성합니다.

**동작 방식**:
*   **모델 로드**: `PPO.load(model_path)`를 사용하여 학습된 모델 체크포인트를 로드합니다.
*   **환경 초기화**: `FightingEnv` 인스턴스를 생성하고, 평가 전용 시드(seed)를 설정합니다.
*   **에피소드 반복**: `N` (예: 100) 에피소드 동안 에이전트를 환경에서 실행합니다.
    *   각 에피소드마다 `env.reset()`을 호출하여 환경을 초기화합니다.
    *   `model.predict(obs, deterministic=True)`를 사용하여 에이전트의 행동을 결정론적으로 얻습니다.
    *   `env.step(action)`을 호출하여 환경을 진행하고 보상, 종료 여부, `info`를 받습니다.
    *   각 에피소드의 총 보상, 길이, 승리 여부 등을 기록합니다.
*   **지표 계산 및 출력**: 모든 에피소드가 끝난 후, 다음 지표들을 계산하여 콘솔에 출력하거나 파일로 저장합니다.
    *   평균 총 보상 (Mean Total Reward)
    *   승률 (Win Rate)
    *   평균 에피소드 길이 (Mean Episode Length)
    *   보상 및 길이의 표준 편차 (Standard Deviation of Reward and Length)

**주요 기능**:
*   `model_path` 인자: 평가할 모델의 경로를 커맨드라인 인자로 받을 수 있도록 합니다.
*   `num_episodes` 인자: 평가할 에피소드 수를 지정할 수 있도록 합니다.
*   `render_mode` (선택 사항): `env.render()`를 호출하여 게임 플레이를 시각적으로 확인할 수 있는 옵션을 제공합니다.

## 4. 통합 테스트 (PyTest/Unittest) 활용

`tests/test_fighting_env.py`와 같은 기존 단위/통합 테스트 파일은 환경의 기본적인 기능(예: `reset`, `step`의 정상 동작, 보상 함수의 계산 정확성, 충돌 처리)을 검증하는 데 사용합니다.

*   **환경의 Gym 표준 준수**: `check_env` 유틸리티(Gymnasium에서 제공)를 사용하여 `FightingEnv`가 Gym 표준을 올바르게 따르는지 확인하는 테스트를 추가할 수 있습니다.
*   **기본적인 에이전트 상호작용**: 무작위 에이전트나 간단한 규칙 기반 에이전트를 사용하여 환경이 예상대로 반응하는지 확인하는 테스트를 포함할 수 있습니다.

이러한 테스트는 에이전트의 성능 평가보다는 환경 자체의 견고성과 정확성을 보장하는 데 중점을 둡니다.

## 5. 추천 구조

*   **학습 중**: `train_rl_agent.py` 스크립트 내에서 `EvalCallback`을 사용하여 주기적으로 에이전트의 성능을 검증하고 TensorBoard에 기록합니다. 이는 학습 진행 상황을 파악하고 최적의 모델을 저장하는 데 필수적입니다.
*   **학습 후**: `evaluate_agent.py` 스크립트를 별도로 생성하여 학습이 완료된 모델의 최종 성능을 정밀하게 측정하고 보고합니다. 이는 모델의 배포 전 최종 검증 단계로 활용됩니다.
*   **개발 중**: `tests/` 폴더 내의 단위/통합 테스트를 통해 환경 및 핵심 컴포넌트의 정확성을 지속적으로 확인합니다.

## 6. 실행 방법 (Execution Methods)

### 6.1. 다중 에이전트 학습 스크립트 실행 (`train_rl_agent.py`)

두 강화학습 에이전트를 동시에 학습시키고, 학습 진행 상황을 로깅합니다.

```bash
python train_rl_agent.py
```

**TensorBoard 확인**: 학습 진행 상황 및 각 에이전트의 검증 결과를 확인하려면, 학습이 시작된 디렉토리에서 다음 명령어를 실행하세요.

```bash
tensorboard --logdir ./logs/ppo_fighting_env_multi_agent
```

### 6.2. 다중 에이전트 평가 스크립트 실행 (`evaluate_agent.py`)

학습된 두 모델을 로드하여 지정된 에피소드 수만큼 평가를 수행하고, 결과를 출력합니다. `--render` 옵션으로 게임 화면을 볼 수 있으며, `--save_gif` 옵션으로 플레이 영상을 GIF로 저장할 수 있습니다.

**예시 1: 화면 렌더링만으로 평가 실행**

```bash
python evaluate_agent.py \
    --model_p1_path ./models/ppo_fighting_env_multi_agent/ppo_p1_final.zip \
    --model_p2_path ./models/ppo_fighting_env_multi_agent/ppo_p2_final.zip \
    --num_episodes 5 --render
```

**예시 2: GIF 파일로 저장하며 평가 실행 (렌더링 포함)**

```bash
python evaluate_agent.py \
    --model_p1_path ./models/ppo_fighting_env_multi_agent/ppo_p1_final.zip \
    --model_p2_path ./models/ppo_fighting_env_multi_agent/ppo_p2_final.zip \
    --num_episodes 1 --render --save_gif --gif_filename multi_agent_evaluation.gif
```

**참고**: `--model_p1_path`와 `--model_p2_path`에는 `train_rl_agent.py` 실행 후 생성된 실제 모델 파일 경로를 지정해야 합니다. 일반적으로 `ppo_p1_final.zip`과 `ppo_p2_final.zip`은 최종 학습 완료 모델입니다.