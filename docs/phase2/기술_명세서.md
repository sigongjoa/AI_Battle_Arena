# AI 파이터 강화학습 기술 명세서

본 문서는 AI 파이터 강화학습 프로젝트의 주요 소프트웨어 컴포넌트인 함수, 클래스, 인터페이스 및 API에 대한 정의와 명세를 제공합니다. 이는 시스템의 구조를 명확히 하고, 개발자들이 일관된 방식으로 코드를 작성하며, 각 컴포넌트 간의 상호작용을 이해하는 데 도움을 주기 위함입니다.

## 1. 인터페이스 정의

### 1.1. `FightingEnv` (OpenAI Gym 호환 환경 인터페이스)

`FightingEnv`는 강화학습 에이전트가 상호작용할 게임 환경을 나타내는 핵심 인터페이스입니다. OpenAI Gym의 표준을 따르며, 에이전트가 환경과 상호작용하는 데 필요한 기본적인 메서드와 속성을 정의합니다.

**목적**: 강화학습 라이브러리(Stable-Baselines3, RLlib 등)와의 원활한 연동을 위한 표준 환경 제공.

**주요 속성 및 메서드**:

*   `action_space`: `gym.spaces.Space` 타입. 에이전트가 취할 수 있는 유효한 행동의 공간을 정의합니다. (예: `Discrete(N)` for N discrete actions, `Box(low, high, shape)` for continuous actions).
    *   **예시**: `Discrete(6)` (정지, 앞, 뒤, 점프, 공격1, 공격2)
*   `observation_space`: `gym.spaces.Space` 타입. 에이전트가 환경으로부터 관측할 수 있는 상태의 공간을 정의합니다. (예: `Box(low, high, shape)` for numerical observations).
    *   **예시**: `Box(low=-1, high=1, shape=(N,))` (정규화된 플레이어 및 상대방 위치, 체력, 상태 등)
*   `reset() -> observation`: 환경을 초기 상태로 재설정하고, 초기 관측(observation)을 반환합니다.
    *   **반환**: `observation` (numpy array 또는 dict): 환경의 초기 상태.
*   `step(action) -> observation, reward, done, info`: 에이전트의 행동을 받아 환경을 한 단계 진행시키고, 다음 관측, 보상, 에피소드 종료 여부, 추가 정보를 반환합니다.
    *   **매개변수**: `action` (int 또는 numpy array): 에이전트가 선택한 행동.
    *   **반환**: 
        *   `observation` (numpy array 또는 dict): 다음 환경 상태.
        *   `reward` (float): 이전 행동에 대한 보상.
        *   `done` (bool): 에피소드가 종료되었는지 여부.
        *   `info` (dict): 디버깅이나 추가 정보(예: 승리 여부, 라운드 시간 등)를 포함하는 딕셔너리.
*   `render(mode='human')`: (선택 사항) 환경을 시각화합니다. `mode='human'`일 경우 화면에 표시하고, 다른 모드일 경우 렌더링된 데이터를 반환할 수 있습니다.
*   `close()`: (선택 사항) 환경 리소스를 정리합니다.

## 2. 클래스 정의

### 2.1. `RewardCalculator` 클래스

`FightingEnv` 내에서 에이전트의 행동과 환경 상태 변화에 따른 보상을 계산하는 역할을 담당합니다. 다양한 보상 요소를 통합하여 최종 보상을 산출합니다.

**목적**: 강화학습 에이전트의 학습 목표에 부합하는 정교한 보상 시스템 구현.

**주요 메서드**:

*   `calculate_reward(player_state, opponent_state, game_info) -> float`: 현재 게임 상태를 기반으로 보상을 계산하여 반환합니다.
    *   **매개변수**: 
        *   `player_state` (dict): 현재 플레이어의 상태 (체력, 위치 등).
        *   `opponent_state` (dict): 상대방의 상태 (체력, 위치 등).
        *   `game_info` (dict): 게임 전반에 대한 정보 (라운드 종료 여부, 승패 등).
    *   **반환**: `reward` (float): 계산된 총 보상 값.

**내부 보상 요소 (예시)**:

*   `_distance_reward(player_pos, opponent_pos) -> float`: 플레이어와 상대방 간의 거리에 따른 보상.
*   `_health_difference_reward(player_health, opponent_health) -> float`: 체력 차이에 따른 보상.
*   `_win_loss_reward(is_round_over, player_won) -> float`: 라운드 종료 시 승패에 따른 보상.
*   `_attack_success_reward(attack_hit) -> float`: 공격 성공 시 보상.

### 2.2. `SimulationManager` 클래스

강화학습 시뮬레이션의 안정성과 재현성을 관리하는 클래스입니다. FPS 고정, 랜덤 시드 관리, 학습 로그 수집 등의 기능을 제공합니다.

**목적**: 일관된 학습 환경을 제공하고, 학습 과정 및 결과 분석을 위한 데이터 수집 자동화.

**주요 속성 및 메서드**:

*   `__init__(fps, seed)`: 시뮬레이션의 FPS와 랜덤 시드를 설정하여 초기화합니다.
    *   **매개변수**: 
        *   `fps` (int): 시뮬레이션의 목표 프레임 속도.
        *   `seed` (int): 랜덤 시드 값.
*   `set_seed(seed)`: 시뮬레이션 전반에 걸쳐 랜덤 시드를 설정합니다.
*   `start_logging(log_dir)`: 학습 로그(보상 곡선, 승률 등)를 지정된 디렉토리에 기록하기 시작합니다.
*   `log_episode_data(episode_data)`: 한 에피소드의 데이터를 로그에 추가합니다.
*   `get_fixed_timestep() -> float`: 고정된 타임스텝 값을 반환합니다.

### 2.3. `CustomTensorboardCallback` 클래스

Stable-Baselines3의 `BaseCallback`을 상속받아 학습 중 TensorBoard에 커스텀 지표를 로깅하는 클래스입니다.

**목적**: 학습 과정에서 에피소드별 총 보상, 에피소드 길이, 승리 여부 등 `FightingEnv`의 `info` 딕셔너리에서 얻을 수 있는 추가적인 정보를 TensorBoard에 기록하여 학습 진행 상황을 상세히 모니터링합니다.

**주요 메서드**:

*   `_on_step() -> bool`: 각 환경 스텝 후에 호출됩니다. 에피소드가 종료될 때 `info` 딕셔너리에서 `episode` 키와 `player_won` 키를 확인하여 TensorBoard에 로깅합니다.

### 2.4. `EvaluationAgent` 클래스 (예정)

학습이 완료된 모델을 로드하여 실제 환경에서 평가를 수행하고, 그 결과를 집계하여 보고하는 역할을 담당하는 클래스입니다.

**목적**: 학습된 에이전트의 최종 성능을 객관적으로 측정하고, 다양한 모델 체크포인트 간의 성능 비교를 용이하게 합니다.

**주요 속성 및 메서드**:

*   `__init__(model_path: str, env_creator: Callable[[], gym.Env], seed: int = None)`: 평가할 모델의 경로와 환경 생성 함수를 받아 초기화합니다.
    *   **매개변수**: 
        *   `model_path` (str): 로드할 학습된 모델의 경로.
        *   `env_creator` (Callable[[], gym.Env]): `FightingEnv` 인스턴스를 생성하는 함수 (예: `lambda: FightingEnv(seed=s)`).
        *   `seed` (int): 평가 환경의 랜덤 시드.
*   `evaluate(num_episodes: int, render: bool = False) -> Dict[str, Any]`: 지정된 에피소드 수만큼 모델을 실행하고 평가 지표를 반환합니다.
    *   **매개변수**: 
        *   `num_episodes` (int): 평가를 수행할 에피소드 수.
        *   `render` (bool): 평가 중 게임 화면을 렌더링할지 여부.
    *   **반환**: `evaluation_results` (Dict[str, Any]): 평균 보상, 승률, 평균 에피소드 길이 등 평가 결과를 포함하는 딕셔너리.

## 3. API 정의 (내부 모듈 간 통신)

### 3.1. `GameEngine` <-> `FightingEnv` API

`FightingEnv`는 실제 게임 엔진(`GameEngine` 또는 `game.py` 모듈)과 상호작용하여 게임 상태를 가져오고, 행동을 적용하며, 결과를 받습니다.

**목적**: 강화학습 환경과 실제 게임 로직 간의 명확한 분리 및 통신 규약 정의.

**주요 호출 및 데이터 흐름**:

*   **`FightingEnv.reset()`**: `GameEngine.reset_game_state()` 호출하여 게임 초기화.
    *   `GameEngine.get_current_state()` 호출하여 초기 관측 데이터 반환.
*   **`FightingEnv.step(action)`**: 
    *   `GameEngine.apply_action(action)` 호출하여 게임 엔진에 행동 전달.
    *   `GameEngine.update_game_logic(timestep)` 호출하여 게임 로직 업데이트.
    *   `GameEngine.get_current_state()` 호출하여 다음 관측 데이터 획득.
    *   `GameEngine.get_game_status()` 호출하여 라운드 종료 여부, 승패 등 획득.

### 3.2. `AI_Agent` <-> `FightingEnv` API

강화학습 에이전트(`AI_Agent` 클래스 또는 학습 라이브러리)는 `FightingEnv` 인터페이스를 통해 환경과 상호작용합니다.

**목적**: 에이전트와 환경 간의 표준화된 상호작용.

**주요 호출 및 데이터 흐름**:

*   **`agent.train(env)`**: 에이전트 학습 시작. 내부적으로 `env.reset()` 및 `env.step()` 반복 호출.
*   **`agent.predict(observation)`**: 학습된 에이전트가 현재 관측을 기반으로 행동을 예측.

### 3.3. `EvalCallback` <-> `FightingEnv` / `Model` API

Stable-Baselines3의 `EvalCallback`은 학습 중 주기적으로 에이전트의 성능을 평가하기 위해 `eval_env`와 `model`과 상호작용합니다.

**목적**: 학습 중 에이전트의 성능을 주기적으로 평가하고, 최고 성능 모델을 저장하며, TensorBoard에 평가 지표를 로깅합니다.

**주요 호출 및 데이터 흐름**:

*   **`EvalCallback` 초기화**: `eval_env` (별도의 `FightingEnv` 인스턴스)와 `model`을 참조합니다.
*   **`_on_step()` (내부)**: `eval_freq`마다 다음을 수행합니다.
    *   `eval_env.reset()` 호출.
    *   `model.predict(obs, deterministic=True)`를 사용하여 행동을 얻고 `eval_env.step(action)`을 반복 호출하여 `n_eval_episodes`만큼 에피소드를 플레이합니다.
    *   수집된 에피소드 보상, 길이, `info` 딕셔너리(승리 여부 포함)를 기반으로 평균 보상, 평균 에피소드 길이, 승률 등을 계산합니다.
    *   `self.logger.record()`를 사용하여 계산된 평가 지표를 TensorBoard에 로깅합니다 (예: `eval/mean_reward`, `eval/mean_ep_length`).
    *   현재 모델의 성능이 `best_mean_reward`를 초과하면 `model.save()`를 호출하여 `best_model_save_path`에 모델을 저장합니다.

## 4. 데이터 구조 (예시)

### 4.1. `PlayerState` (딕셔너리 또는 클래스)

플레이어의 현재 상태를 나타내는 데이터 구조.

*   `health`: 현재 체력 (float, 0.0 ~ 1.0)
*   `position`: (x, y) 좌표 (tuple of float)
*   `velocity`: (vx, vy) 속도 (tuple of float)
*   `is_attacking`: 공격 중 여부 (bool)
*   `is_blocking`: 방어 중 여부 (bool)
*   `is_hit`: 피격 여부 (bool)
*   `facing_direction`: 바라보는 방향 (int, -1: 왼쪽, 1: 오른쪽)

### 4.2. `GameInfo` (딕셔너리 또는 클래스)

게임 전반에 대한 정보를 나타내는 데이터 구조.

*   `round_over`: 라운드 종료 여부 (bool)
*   `player_won`: 현재 플레이어의 승리 여부 (bool, `round_over`가 True일 때 유효)
*   `time_left`: 남은 라운드 시간 (float)

### 4.3. `EvaluationResults` (딕셔너리 또는 클래스) (예정)

`EvaluationAgent` 클래스의 `evaluate` 메서드에서 반환될 평가 결과 데이터 구조.

*   `mean_reward`: 평균 에피소드 보상 (float)
*   `std_reward`: 에피소드 보상의 표준 편차 (float)
*   `win_rate`: 승률 (float, 0.0 ~ 1.0)
*   `mean_episode_length`: 평균 에피소드 길이 (float)
*   `std_episode_length`: 에피소드 길이의 표준 편차 (float)

## 5. 함수 정의 (유틸리티)

### 5.1. `normalize_observation(raw_observation) -> normalized_observation`

**목적**: 게임 엔진에서 얻은 원시 관측 데이터를 강화학습 모델이 처리하기 용이한 형태로 정규화합니다.

*   **매개변수**: `raw_observation` (dict 또는 numpy array): 게임 엔진으로부터 직접 얻은 관측 데이터.
*   **반환**: `normalized_observation` (numpy array): -1과 1 사이로 정규화된 관측 데이터.

### 5.2. `denormalize_action(normalized_action) -> game_action`

**목적**: 강화학습 모델이 출력한 정규화된 행동을 게임 엔진이 이해할 수 있는 실제 게임 행동으로 변환합니다.

*   **매개변수**: `normalized_action` (int 또는 numpy array): 모델이 출력한 행동.
*   **반환**: `game_action` (int 또는 dict): 게임 엔진에 적용할 수 있는 행동.

### 5.3. `run_evaluation(model, env, num_episodes, render=False) -> Dict[str, Any]` (예정)

**목적**: 주어진 모델과 환경을 사용하여 지정된 수의 에피소드 동안 평가를 실행하고, 집계된 평가 지표를 반환합니다. `evaluate_agent.py` 스크립트에서 활용됩니다.

*   **매개변수**: 
    *   `model`: 평가할 학습된 Stable-Baselines3 모델 인스턴스.
    *   `env`: 평가에 사용할 `FightingEnv` 인스턴스.
    *   `num_episodes` (int): 실행할 에피소드 수.
    *   `render` (bool): 각 스텝마다 환경을 렌더링할지 여부.
*   **반환**: `evaluation_results` (Dict[str, Any]): `EvaluationResults` 데이터 구조와 유사한 형태의 평가 지표 딕셔너리.