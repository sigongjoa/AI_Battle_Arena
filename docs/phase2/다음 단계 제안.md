# 다음 단계 제안

## 리워드 설계 확정

현재는 단순히 idle=0, 공격 성공 시 +reward 구조이지만, 강화학습을 위해서는 다음과 같은 구조를 잡아야 합니다:

*   **거리 보상**: 상대와 가까워지면 양의 보상, 멀어지면 음의 보상
*   **체력 차이 보상**: 상대보다 내 체력이 많으면 양의 보상
*   **승리/패배 보상**: 라운드 종료 시 크게 양/음의 보상

## Gym 환경 래핑

`FightingEnv`를 OpenAI Gym 스타일로 맞춰야 합니다.

*   `reset()`: 환경 초기화
*   `step()`: 한 스텝 진행
*   `action_space`: 가능한 행동 공간 정의
*   `observation_space`: 관측 공간 정의

이를 통해 RL 라이브러리 (예: Stable-Baselines3, RLlib, CleanRL)와 바로 연결할 수 있습니다.

## Baseline 에이전트 학습

처음에는 PPO나 DQN 같은 표준 알고리즘으로 간단히 학습을 진행합니다.
이를 통해 "캐릭터가 움직이고, 공격하고, 상대 체력을 깎는" 정도까지 학습되는지 확인합니다.

## 시뮬레이션 안정화

*   FPS, step size, 랜덤성(seeding) 고정
*   로그 수집 (보상 곡선, 승률) 자동화

## 고급 모델 연결