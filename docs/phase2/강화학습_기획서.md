# AI 파이터 강화학습 프로젝트 기획서

## 1. 프로젝트 개요

본 프로젝트는 격투 게임 환경 내에서 강화학습(Reinforcement Learning, RL) 기반의 AI 파이터를 개발하고 학습시키는 것을 목표로 합니다. 기존의 단순한 규칙 기반 AI를 넘어, 복잡한 게임 상황에 유연하게 대처하고 전략적인 플레이를 수행할 수 있는 지능형 에이전트를 구현하고자 합니다.

## 2. 프로젝트 목표

*   OpenAI Gym 표준을 따르는 강화학습 환경 구축
*   다양한 보상 설계를 통한 효율적인 학습 유도
*   표준 강화학습 알고리즘을 활용한 Baseline AI 파이터 학습 및 검증
*   안정적인 시뮬레이션 환경 구축 및 학습 데이터 자동화
*   최종적으로 고급 강화학습 모델을 적용하여 고성능 AI 파이터 개발

## 3. 주요 추진 단계

### 3.1. 리워드 설계 확정

강화학습 에이전트의 효율적인 학습을 위해 현재의 단순한 보상 구조(idle=0, 공격 성공 시 +reward)를 확장하고 구체화합니다.

*   **거리 보상**: 상대방과의 거리에 따라 보상을 차등 지급 (가까워지면 양의 보상, 멀어지면 음의 보상).
*   **체력 차이 보상**: 자신과 상대방의 체력 차이에 기반한 보상 (자신이 유리하면 양의 보상).
*   **승리/패배 보상**: 라운드 종료 시 승패에 따라 큰 폭의 보상 또는 패널티를 부여.

### 3.2. Gym 환경 래핑

현재의 `FightingEnv`를 OpenAI Gym 인터페이스 표준에 맞춰 래핑합니다. 이를 통해 다양한 강화학습 라이브러리와의 호환성을 확보하고 개발 효율성을 높입니다.

*   `reset()` 메서드 구현: 환경을 초기 상태로 재설정.
*   `step()` 메서드 구현: 에이전트의 행동을 받아 환경을 한 단계 진행시키고, 다음 상태, 보상, 종료 여부, 추가 정보를 반환.
*   `action_space` 정의: 에이전트가 취할 수 있는 행동 공간을 명확히 정의 (예: `Discrete`, `Box`).
*   `observation_space` 정의: 에이전트가 관측할 수 있는 상태 공간을 명확히 정의 (예: `Box`).

이 과정을 통해 Stable-Baselines3, RLlib, CleanRL 등 널리 사용되는 강화학습 라이브러리를 즉시 적용할 수 있는 기반을 마련합니다.

### 3.3. Baseline 에이전트 학습

Gym 환경 래핑이 완료되면, PPO(Proximal Policy Optimization) 또는 DQN(Deep Q-Network)과 같은 표준 강화학습 알고리즘을 사용하여 Baseline 에이전트를 학습시킵니다. 이 단계의 목표는 다음과 같습니다.

*   AI 파이터가 기본적인 움직임(이동, 점프 등)을 학습하는지 확인.
*   공격 행동을 학습하여 상대방의 체력을 감소시킬 수 있는지 확인.
*   강화학습 환경 및 보상 설계의 유효성을 초기 검증.

### 3.4. 시뮬레이션 안정화

강화학습의 안정적인 진행과 결과의 신뢰성 확보를 위해 시뮬레이션 환경을 최적화합니다.

*   **FPS 및 Step Size 고정**: 학습 과정의 일관성을 위해 게임의 초당 프레임(FPS)과 시뮬레이션 스텝 크기를 고정.
*   **랜덤성(Seeding) 고정**: 실험의 재현성을 위해 모든 랜덤 요소에 시드(seed)를 적용하여 고정.
*   **로그 수집 자동화**: 학습 과정에서 보상 곡선, 승률, 에피소드 길이 등 주요 지표를 자동으로 수집하고 시각화하는 시스템 구축.

### 3.5. 고급 모델 연결

Baseline 에이전트 학습 및 시뮬레이션 안정화가 완료되면, 더 복잡하고 성능이 우수한 강화학습 모델(예: SAC, TD3, MuZero 등)을 적용하여 AI 파이터의 성능을 고도화합니다. 이 단계에서는 더욱 정교한 전략 학습과 인간 수준에 근접하는 플레이를 목표로 합니다.

## 4. 기대 효과

*   강화학습 기반의 지능형 AI 파이터 개발을 통한 게임 플레이 경험 향상.
*   모듈화된 Gym 환경 구축으로 다양한 강화학습 알고리즘의 손쉬운 적용 가능.
*   안정적인 학습 환경을 통해 연구 및 개발 효율성 증대.
*   향후 AI 파이터의 성능 개선 및 확장 가능성 확보.

## 5. 향후 계획

본 기획서에 제시된 단계를 성공적으로 완료한 후, 다음과 같은 추가적인 개선 및 확장을 고려할 수 있습니다.

*   멀티 에이전트 강화학습 도입 (2인 대전 학습).
*   모방 학습(Imitation Learning)을 통한 인간 플레이 데이터 활용.
*   메타 학습(Meta-Learning)을 통한 다양한 캐릭터 및 환경 적응력 강화.
*   실제 게임 엔진과의 연동 및 최적화.