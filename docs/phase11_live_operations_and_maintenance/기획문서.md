# AI 복싱 페르소나 구현 전략: '스타일'을 학습하는 AI

## 1. 개요

본 문서는 기존의 '일반적인 복서' AI 모델을 넘어, 특정 전투 스타일(아웃복서, 인파이터, 슬러거 등)을 가진 구체적인 AI 페르소나를 육성하기 위한 심화된 전략을 제안합니다. 이는 보편적인 규칙 생성을 넘어, 특정 철학을 가진 '선수'를 육성하는 과정과 같습니다. 최신 AI 연구 이론을 접목하여, **'보상 함수 엔지니어링'**과 **'모방 학습 및 자기 대국'**이라는 두 가지 핵심 전략을 통해 이를 구현하고자 합니다.

---

## 2. 전략 1: 스타일별 '보상 함수' 설계를 통한 페르소나 조각

각 복싱 스타일이 선호하는 전투 거리, 공격 패턴, 방어 방식의 차이를 강화학습의 '보상'과 '패널티'로 직접 연결하여 AI의 행동을 정교하게 유도합니다.

### 2.1. 🥊 아웃복서 (Out-fighter) 페르소나: '거점'을 사수하고 점수를 쌓는 AI

-   **핵심 철학:** 긴 사거리를 활용해 상대의 접근을 차단하고, 잽(Jab)으로 점수를 쌓아 판정승을 노립니다.
-   **보상 함수 설계:**
    -   **거리 보상 (핵심):**
        -   자신의 잽 사거리 끝부분을 '최적 거리(Sweet Spot)'로 설정하고, 이 거리 유지 시 높은 보상 **(+0.5)**을 지속적으로 부여합니다.
        -   상대방이 최적 거리 안으로 진입 시 (위험 지역), 보상을 급격히 감소시키거나 패널티 **(-0.3)**를 부여합니다.
    -   **공격 보상:**
        -   잽(Jab) 명중 시 다른 공격보다 높은 보상 **(+1.0)**을 부여합니다.
        -   위험도가 높은 공격(예: 훅)이 빗나갔을 때 큰 패널티 **(-0.5)**를 부여합니다.
    -   **방어 보상:**
        -   백스텝, 사이드스텝을 이용한 회피 성공 시 보상 **(+0.2)**을 부여합니다.

### 2.2. 🥊 인파이터 (In-fighter) 페르소나: 파고들어 압박하는 AI

-   **핵심 철학:** 방어 기술(더킹, 위빙)로 상대 공격을 무력화하며 파고든 후, 짧고 빠른 연타로 승부합니다.
-   **보상 함수 설계:**
    -   **거리 보상 (핵심):**
        -   상대방의 품 안(초근접 거리)을 '최적 거리'로 설정하고, 해당 거리 유지 시 가장 높은 보상 **(+0.5)**을 부여합니다.
        -   거리가 멀어질수록 강한 패널티 **(-0.4)**를 부여하여 지속적인 압박을 유도합니다.
    -   **공격 보상:**
        -   어퍼컷, 바디블로 등 근거리 공격 명중 시 높은 보상 **(+1.2)**을 부여합니다.
        -   `RhythmAnalyzer`를 활용, 짧은 시간 내 연속 공격(행동 밀도) 성공 시 추가 보너스를 제공합니다.
    -   **방어 보상:**
        -   더킹, 위빙 등 상체 움직임으로 공격 회피 성공 시 매우 큰 보상 **(+0.7)**을 부여하여 파고드는 능력을 직접적으로 학습시킵니다.

### 2.3. 🥊 슬러거 (Slugger) 페르소나: 한 방을 노리는 AI

-   **핵심 철학:** 정교함 대신, 한 번의 강력한 공격으로 경기를 뒤집는 것을 목표로 합니다.
-   **보상 함수 설계:**
    -   **거리 보상:** 특정 거리 유지에 대한 보상을 최소화하여, AI가 거리 조절보다 기회를 포착하는 데 집중하도록 합니다.
    -   **공격 보상 (핵심):**
        -   잽과 같은 가벼운 공격의 보상은 낮게 **(+0.1)** 설정합니다.
        -   강력한 한 방(헤비 훅, 슈퍼 펀치) 명중 시 막대한 보상 **(+5.0)**을 부여합니다.
        -   카운터 펀치(상대의 공격 중 적중) 성공 시 추가 보너스 **(+3.0)**를 부여합니다.
    -   **방어/체력 보상:**
        -   자신의 체력 감소에 대한 패널티를 다른 페르소나보다 낮게 설정하여, '맷집'을 바탕으로 위험을 감수하는 슬러거의 특성을 반영합니다.

---

## 3. 전략 2: 최신 이론의 '모방'과 '진화' (Imitation & Self-Play)

보상 함수만으로 구현하기 어려운 인간 고유의 복잡한 전략은, '모방 후 스스로 발전'하는 접근 방식을 적용합니다.

### 3.1. 1단계: 모방 학습 (Imitation Learning)으로 기본기 장착

-   **실행 방안:**
    1.  **데이터 수집:** 특정 스타일(예: 프로 선수의 아웃복싱)의 경기 영상을 수집합니다.
    2.  **데이터 변환:** Pose Estimation 기술을 사용하여 영상 속 선수의 움직임을 게임 내 (상태, 행동) 데이터셋으로 변환합니다.
    3.  **사전 학습 (Pre-training):** 강화학습 전, 지도 학습(Supervised Learning)을 통해 AI가 수집된 데이터셋을 모방하도록 학습시킵니다.
-   **기대 효과:** AI가 맨땅에서 시작하는 대신, 특정 스타일의 기본기를 갖춘 상태에서 강화학습을 시작하여 학습 효율을 극대화합니다.

### 3.2. 2단계: 자기 대국 (Self-Play)과 '호기심'으로 한계 돌파

-   **실행 방안:**
    1.  **스타일 대전 (Self-Play):** 모방 학습을 마친 '아웃복서 AI'와 '인파이터 AI'를 서로 수만 번 대전시킵니다. 각 AI는 자신의 보상 함수를 따르면서도, 상대를 이기기 위해 전략을 끊임없이 수정하고 발전시킵니다.
    2.  **'호기심' 주입:** `Curiosity-Driven Exploration` 모듈을 활성화하여, AI가 기존에 시도하지 않은 새로운 행동이나 패턴을 보일 때마다 추가 보상을 부여합니다.
-   **기대 효과:**
    -   AI는 단순히 인간을 모방하는 것을 넘어, 각 스타일에 대한 최적의 대응 전략, 즉 '메타(Meta)'를 스스로 구축하고 진화시킵니다.
    -   이를 통해 아웃복서는 인파이터를 상대하기 위한 새로운 거리 조절법을 발견하는 등, 예측 불가능한 창의적인 플레이 스타일을 창조할 수 있습니다.
