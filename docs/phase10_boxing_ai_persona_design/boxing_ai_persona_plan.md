# Phase 10: 복싱 AI 페르소나 설계 계획 (v2.0)

## 1. 개요

본 문서는 2D 격투 게임에 실제 복싱 이론을 적용하여, 단순히 승리하는 AI가 아닌 **아웃복서, 인파이터, 슬러거와 같은 구체적인 스타일을 가진 AI 페르소나**를 학습시키기 위한 심화된 실행 계획을 정의한다. 이는 보편적인 규칙을 넘어, 특정 철학을 가진 '선수'를 육성하는 과정과 같으며, 최신 논문의 아이디어를 포함하여 각 스타일을 학습시키기 위한 두 가지 핵심 전략을 제안한다.

## 2. 전략 1: 스타일별 '보상 함수'를 설계하여 페르소나를 조각하기

각 복싱 스타일은 선호하는 전투 거리, 공격 패턴, 방어 방식이 모두 다르다. 이 선호도를 강화학습의 '보상'과 '패널티'로 직접 연결하여 AI의 행동을 유도할 수 있다. 이미 설계된 'AI 페르소나' 아키텍처는 이를 위한 완벽한 실험대이다.

### 2.1. 🥊 아웃복서 (Out-fighter) 페르소나: '거점'을 사수하고 점수를 쌓는 AI

-   **핵심 철학:** 긴 사거리를 활용해 상대방을 들어오지 못하게 막고, 잽으로 점수를 쌓아 판정승을 노린다.
-   **보상 함수 설계:**
    -   **거리 보상 (핵심):**
        -   내 잽 사거리의 끝부분을 '최적 거리(Sweet Spot)'로 설정하고, 이 거리를 유지할 때 가장 높은 보상(`+0.5`)을 지속적으로 부여한다.
        -   상대방이 이 거리 안으로 들어오면 (위험 지역), 보상을 급격히 감소시키거나 패널티(`-0.3`)를 부여한다.
    -   **공격 보상:**
        -   잽(Jab) 공격이 명중했을 때 다른 공격보다 높은 보상(`+1.0`)을 준다.
        -   강력하지만 빈틈이 큰 공격(예: 훅)이 빗나갔을 때 큰 패널티(`-0.5`)를 부여한다.
    -   **방어 보상:**
        -   백스텝이나 사이드스텝으로 상대의 공격을 회피했을 때 보상(`+0.2`)을 준다.

### 2.2. 🥊 인파이터 (In-fighter) 페르소나: 파고들어 압박하는 AI

-   **핵심 철학:** 방어 기술(더킹, 위빙)을 이용해 상대의 공격을 피하며 파고든 후, 짧고 빠른 연타로 승부한다.
-   **보상 함수 설계:**
    -   **거리 보상 (핵심):**
        -   상대방의 품 안(초근접 거리)을 '최적 거리'로 설정하고, 이 거리에 머무를 때 가장 높은 보상(`+0.5`)을 부여한다.
        -   거리가 멀어질수록 강한 패널티(`-0.4`)를 부여하여 AI가 끊임없이 파고들도록 강제한다.
    -   **공격 보상:**
        -   어퍼컷, 바디블로 등 근거리 전용 공격이 명중했을 때 높은 보상(`+1.2`)을 준다.
        -   `RhythmAnalyzer`를 활용하여 짧은 시간 안에 여러 번의 공격이 연속으로 명중(행동 밀도 지표)했을 때 추가 보너스를 부여한다.
    -   **방어 보상:**
        -   더킹, 위빙과 같은 상체 움직임으로 상대의 공격을 회피하는 데 성공했을 때 매우 큰 보상(`+0.7`)을 준다. 이는 파고드는 능력을 직접적으로 학습시킨다.

### 2.3. 🥊 슬러거 (Slugger) 페르소나: 한 방을 노리는 AI

-   **핵심 철학:** 정교함은 부족하지만, 한 번의 강력한 공격으로 경기를 뒤집는 것을 목표로 한다.
-   **보상 함수 설계:**
    -   **거리 보상:** 거리 자체에는 큰 보상을 주지 않는다. AI가 거리를 조절하기보다는 기회를 노리도록 한다.
    -   **공격 보상 (핵심):**
        -   잽이나 연타 같은 가벼운 공격의 보상은 낮게(`+0.1`) 설정한다.
        -   대신 강력한 한 방(예: 헤비 훅, 슈퍼 펀치)이 명중했을 때 엄청나게 높은 보상(`+5.0`)을 부여한다.
        -   카운터 펀치 성공 시(상대가 공격하는 도중에 내 공격이 적중) 추가적인 보너스(`+3.0`)를 준다.
    -   **방어/체력 보상:**
        -   자신의 체력이 깎이는 것에 대한 패널티를 다른 페르소나보다 낮게 설정한다. 이는 '맷집'을 바탕으로 한 방을 노리는 슬러거의 특성을 반영한다. (위험 감수 성향 증가)

## 3. 전략 2: 최신 이론을 '모방'하고 '진화'시키기 (Imitation & Self-Play)

최신 논문에서 제안하는 복잡한 전략이나 인간 고수의 플레이는, 보상 함수만으로 만들어내기 어려울 수 있다. 이럴 때는 '일단 따라하게 하고, 그 뒤에 스스로 발전하게 하는' 접근 방식이 유효하다.

### 3.1. 1단계: 모방 학습 (Imitation Learning)으로 기본기 장착

-   **실행 방안:**
    -   **데이터 수집:** 학습시키고 싶은 스타일(예: 특정 프로 선수의 아웃복싱)의 플레이 영상을 수집한다.
    -   **데이터 변환:** Pose Estimation 기술 등을 활용하여 영상 속 플레이어의 움직임을 게임 내 (상태, 행동) 데이터셋으로 변환한다.
    -   **사전 학습 (Pre-training):** 본격적인 강화학습 전에, 이 데이터셋을 이용해 AI가 인간의 플레이를 그대로 따라 하도록 지도 학습(Supervised Learning)시킵니다.
-   **효과:** AI는 처음부터 맨땅에 헤딩하는 것이 아니라, 특정 스타일의 기본 움직임과 공격 패턴을 어느 정도 갖춘 상태에서 강화학습을 시작할 수 있다.

### 3.2. 2단계: 자기 대국 (Self-Play)과 '호기심'으로 한계를 돌파하기

인간을 모방하는 것만으로는 인간을 뛰어넘을 수 없다. AI가 자신만의 해법을 찾도록 해야 한다.

-   **실행 방안:**
    -   **스타일 대전:** 모방 학습으로 기본기를 익힌 '아웃복서 AI'와 '인파이터 AI'를 서로 수만 번 대전시킨다 (`Self-Play`). 각자는 자신의 스타일을 유지하기 위한 보상 함수를 따르면서도, 상대방을 이기기 위해 전략을 수정해 나간다.
    -   **'호기심' 주입:** `구현_알고리즘_명세서.md`에 정의된 `Curiosity-Driven Exploration` 모듈을 활성화한다. AI가 기존에 시도하지 않았던 새로운 행동이나 패턴을 보일 때마다 추가적인 보상을 준다.
-   **효과:**
    -   아웃복서는 인파이터를 상대하기 위해 새로운 거리 조절법이나 콤비네이션을 발견할 수 있다.
    -   AI는 단순히 인간의 플레이를 따라 하는 것을 넘어, 특정 스타일에 대한 '메타(Meta)' 를 스스로 구축하고 진화시키게 된다. 최신 논문의 복잡한 상호작용 이론이 있다면, 이 `Self-Play` 환경에서 자연스럽게 발현될 가능성이 높다.

## 4. 결론: 복싱 AI 페르소나의 탄생

이 두 가지 전략을 통해, AI는 이제 '일반적인 복서'를 넘어, 뚜렷한 개성과 철학을 가진 다양한 스타일의 '선수'로 거듭날 수 있을 것이다. 이는 프로젝트가 추구하는 깊이 있는 게임성과 독창적인 AI를 만드는 핵심 열쇠가 될 것이다.
