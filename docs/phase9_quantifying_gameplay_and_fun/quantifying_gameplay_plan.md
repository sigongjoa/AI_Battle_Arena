# Phase 9: 게임성 정량화 및 AI 탐색 계획

## 1. 개요

본 문서는 게임의 핵심 가치인 '재미'를 정량화하고, 강화학습(RL) 에이전트가 이를 탐색하도록 하는 구체적인 계획을 정의한다. '재미'는 이미 '손맛(Game Feel)'과 '전투의 리듬(Battle Rhythm)'이라는 개념으로 정의되었으며, 이를 측정하고 탐색하는 두 단계로 접근한다.

## 2. 측정 단계: '재미'라는 감정을 데이터로 번역하기

'재미'의 실체는 **"플레이어의 기대가 만족스러운 방식으로 실현되는 경험"**으로 정의하며, 이를 데이터로 측정하는 과정은 다음과 같다.

### 2.1. 1단계 - 물리적 피드백 측정
- **내용:** '손맛'의 가장 기본적인 요소인 히트스톱, 카메라 셰이크 강도 등을 '대리 변수(Proxy Variable)'로 측정한다.
- **관련 문서:** `구현_알고리즘_명세서.md`의 '손맛' (Immersion) 점수화 알고리즘.

### 2.2. 2단계 - '리듬'과 '맥락' 측정
- **내용:** 단순한 타격의 순간을 넘어, 전투의 서사(Narrative)를 구성하는 '리듬'과 '맥락'을 측정한다.
  - 예시: "아슬아슬하게 피하고 때렸을 때의 쾌감"(`CounterPlayScore`), "공격이 들어가기까지의 긴장감"(`TensionIndex`).
- **관련 문서:** `구현_알고리즘_명세서.md`의 '리듬' 기반 지표 추가.

### 2.3. 3단계 - 인간의 선호도 학습 (RLHF)
- **내용:** 궁극적으로 '재미'는 주관적이므로, 인간의 선호도 데이터를 직접 학습하여 정량화한다.
  - **방법:** 여러 플레이 영상을 A/B 테스트 형식으로 사람들에게 보여주고, "어느 쪽이 더 마음에 드나요?"라고 물어 그 선호도 데이터를 학습한 보상 모델 `R_θ`를 생성한다.
- **관련 문서:** `구현_알고리즘_명세서.md`의 RLHF 방식 고도화 (A/B 테스트).

## 3. 탐색 단계: '재미있는 플레이'를 AI에게 발견하게 하기

'재미'가 보상 모델 `R_θ`로 정량화되었다면, 이제 AI는 단순히 '승리'를 위해 싸우는 것이 아니라, '재미있게 이기기 위해' 싸우게 된다.

### 3.1. AI의 Self-Play가 곧 게임성 탐색
- **내용:** '프로게이머' 페르소나에게 '승리' 보상과 함께 이 '재미' 보상 모델을 함께 적용하여 학습시킨다.
- **기대 효과:** AI는 가장 효율적인 콤보를 찾을 뿐만 아니라, 그중에서도 인간이 가장 '재미있다'고 평가한 리듬과 패턴을 가진 콤보를 스스로 발견하고 사용하기 시작할 것이다.

### 3.2. XAI 리포트를 통한 인간과의 협업
- **내용:** AI가 자율 학습을 통해 발견한 '재미있는 패턴'은 XAI 리포트를 통해 개발자에게 전달된다.
- **리포트 예시:** "발견: '슬라이딩' 이후 특정 프레임에 '강펀치'를 사용했을 때, '카운터플레이 점수'가 비정상적으로 높게 측정되었습니다. 이 '슬라이딩 캔슬 강펀치'를 의도된 기술로 승격시키는 것을 검토해볼 수 있습니다."
- **기대 효과:** AI가 발견한 우연한 '버그'나 패턴을 개발자가 '게임성'으로 승화시키는, AI와 인간의 협업 과정이 된다.

## 4. 결론

RL Policy와 게임성을 찾는 여정은 이미 상세하게 설계된 `Phase 7`의 v2.0 계획을 하나씩 실험하고, 결과를 분석하고, 다음 실험을 설계하는 과정 그 자체이다. 이제 거대한 설계를 잠시 내려두고, 가장 흥미로운 페르소나 하나를 골라 직접 '창조'해 보는 것을 추천한다.
