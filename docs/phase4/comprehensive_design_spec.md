# RL-WebRTC 연동: 종합 설계 명세서

## 1. 데이터 모델 (Data Models)

시스템의 핵심 데이터인 `Observation`과 `Action`의 구체적인 구조를 정의하여 데이터 불일치로 인한 오류를 방지한다.

### 1.1. `Observation` (관측 데이터)

-   **타입**: `number[]` (숫자 배열)
-   **설명**: 프론트엔드 게임 상태를 정규화하여 백엔드 RL 에이전트에 전달하는 데이터. 배열의 각 인덱스는 다음을 의미한다.

| 인덱스 | 내용                     | 데이터 타입 | 정규화 범위 | 설명                                       |
| :----- | :----------------------- | :---------- | :---------- | :----------------------------------------- |
| 0      | 플레이어 P1의 x 좌표     | `float`     | `0.0` ~ `1.0` | 화면 너비에 대한 비율                      |
| 1      | 플레이어 P1의 y 좌표     | `float`     | `0.0` ~ `1.0` | 화면 높이에 대한 비율                      |
| 2      | 플레이어 P1의 체력       | `float`     | `0.0` ~ `1.0` | 최대 체력에 대한 비율                      |
| 3      | 플레이어 P1의 상태       | `int`       | `0` ~ `N`   | 0: Idle, 1: Walking, 2: Jumping, 3: Attacking... |
| 4      | 플레이어 P2(상대)의 x 좌표 | `float`     | `0.0` ~ `1.0` |                                            |
| 5      | 플레이어 P2(상대)의 y 좌표 | `float`     | `0.0` ~ `1.0` |                                            |
| 6      | 플레이어 P2(상대)의 체력 | `float`     | `0.0` ~ `1.0` |                                            |
| 7      | 플레이어 P2(상대)의 상태 | `int`       | `0` ~ `N`   |                                            |
| ...    | 기타 필요한 상태 정보    | `...`       | `...`       | (예: 남은 시간, 프레임 데이터 등)          |

### 1.2. `Action` (행동 데이터)

-   **타입**: `number` (정수)
-   **설명**: 백엔드 RL 에이전트가 결정한 행동을 나타내는 정수값. 각 값은 다음 행동에 매핑된다.

| 값 | 행동         | 설명             |
| :--- | :----------- | :--------------- |
| 0  | `IDLE`       | 가만히 있기      |
| 1  | `MOVE_FWD`   | 앞으로 이동      |
| 2  | `MOVE_BWD`   | 뒤로 이동        |
| 3  | `JUMP`       | 점프             |
| 4  | `ATTACK_1`   | 기본 공격        |
| 5  | `ATTACK_2`   | 특수 공격        |

---

## 2. 아키텍처 및 디자인 패턴

### 2.1. 전체 아키텍처

-   WebRTC 데이터 채널을 통한 P2P(Peer-to-Peer) 통신 구조를 기반으로 한다.
-   **Backend (Python)**: RL 에이전트 및 학습 로직 실행.
-   **Frontend (Browser)**: 게임 시뮬레이션 및 렌더링 실행.
-   **Signaling Server**: 두 Peer 간의 초기 연결 설정을 중개.

*(아키텍처 다이어그램은 `rl_over_webrtc_implementation_plan.md` 문서 참고)*

### 2.2. 상태 관리 (State Management)

-   **Source of Truth**: 게임 상태에 대한 **유일한 진실의 원천(Source of Truth)은 프론트엔드의 게임 엔진(`engine.ts`)**이다.
-   **Backend**: 백엔드는 게임 상태를 저장하지 않는다 (`Stateless`). 매 `step`마다 프론트엔드로부터 `Observation`을 받아 의사결정을 내리고, 그 결과를 잊는다. 이는 시스템의 복잡도를 낮춘다.

### 2.3. 통신 패턴: 동기-비동기 브릿지 (Sync-Async Bridge)

-   백엔드에서 동기적으로 동작하는 `Stable-Baselines3`와 비동기 `aiortc`를 연동하기 위해 **별도 스레드와 큐(Queue)를 사용하는 디자인 패턴**을 채택한다.
-   이는 `backend_webrtc_rl_spec.md`에 명시된 핵심 아키텍처 패턴으로, 시스템의 안정성을 보장한다.

---

## 3. 비즈니스 로직 및 사용자 시나리오

### 시나리오: "AI 자동 학습"

1.  **개발자 (백엔드 실행)**: 터미널에서 `python train_rl_agent.py`를 실행한다.
2.  **백엔드**: `FightingEnv`가 초기화되고, 내부적으로 `WebRTCClient` 스레드가 시작된다. 백엔드는 PeerJS 시그널링 서버에 자신의 Peer ID(예: `backend-agent-01`)를 등록하고 프론트엔드의 연결을 기다린다.
3.  **개발자 (프론트엔드 접속)**: 브라우저를 열고 학습용 URL(예: `http://localhost:5173/?mode=rl_training&backend_peer_id=backend-agent-01`)에 접속한다.
4.  **프론트엔드**: `GameScreen.tsx`가 `rl_training` 모드를 감지하고, URL에서 백엔드 Peer ID를 가져와 `RLAgentController`를 렌더링한다.
5.  **P2P 연결**: `RLAgentController`가 자신의 Peer ID를 시그널링 서버에 등록하고, 백엔드의 Peer ID로 연결을 요청하여 데이터 채널을 수립한다.
6.  **핸드셰이크**: 데이터 채널이 열리면, 프론트엔드는 백엔드로 `{"type": "connection_ready"}` 메시지를 보낸다.
7.  **학습 시작**: 백엔드의 `FightingEnv`는 `connection_ready` 메시지를 받고 대기 상태에서 벗어난다. `model.learn()` 루프가 본격적으로 시작되며, 첫 `reset()` 명령을 프론트엔드로 전송한다.
8.  **학습 루프**: 이후 `action` 전송과 `step_result` 수신이 반복되며 학습이 진행된다.

---

## 4. 예외 처리 및 오류 정책

### 4.1. WebRTC 연결 실패 / 끊김

-   **백엔드 정책**:
    -   `FightingEnv`의 `step` 또는 `reset` 메서드에서 `result_queue.get()` 대기 시, **10초의 타임아웃**을 설정한다.
    -   타임아웃 발생 시, `TimeoutError`를 발생시킨다.
    -   `train_rl_agent.py`는 이 예외를 `try...except`로 잡아 "Connection to frontend timed out." 오류 메시지를 로깅하고, `env.close()`를 호출하여 프로세스를 정상 종료시킨다.
-   **프론트엔드 정책**:
    -   `RLAgentController`에서 데이터 채널의 `close` 이벤트가 감지되면, 3초 간격으로 3회 재연결을 시도한다.
    -   재연결 최종 실패 시, 화면에 "백엔드 트레이너와의 연결이 끊겼습니다." 라는 오류 메시지를 표시한다.

### 4.2. 잘못된 데이터 수신

-   **정책**: 프론트엔드와 백엔드 양측 모두, 수신한 JSON 메시지의 `type` 필드가 API 명세에 없는 값이거나 필수 필드가 누락된 경우, 해당 메시지를 **무시하고 콘솔에 경고(warning)를 로깅**한다. 시스템이 비정상 종료되어서는 안 된다.

### 4.3. 강화학습 중 논리적 오류

-   **정책**: RL 에이전트가 유효 범위를 벗어난 `action` 값(예: `Discrete(6)` 공간에서 7을 생성)을 생성할 경우, `FightingEnv`는 이를 **`IDLE` (0) 액션으로 간주하여 처리**하고, 해당 step의 `info` 딕셔너리에 `{"invalid_action_received": 7}` 과 같은 경고를 포함하여 반환한다.
