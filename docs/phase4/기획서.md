# Phase 4 기획서: WebRTC 기반 강화학습 환경 구축

## 1. 개요

### 1.1. 프로젝트 목표
본 프로젝트의 최종 목표는 **WebRTC를 통해 브라우저 게임과 실시간으로 연동되는 강화학습 에이전트를 개발하고, 이를 통해 인간 플레이어와 유사한 수준의 대전 전략을 자율적으로 학습하는 AI 파이터를 구현하는 것**이다.

### 1.2. 기대 효과
-   **지능적인 게임 플레이 경험 제공**: 예측 불가능하고 전략적인 AI 상대를 통해 사용자의 게임 경험을 향상시킨다.
-   **지속적인 콘텐츠 확장**: 새로운 캐릭터나 밸런스 패치에 능동적으로 적응하는 AI를 통해 콘텐츠의 수명을 연장한다.
-   **개발 효율성 증대**: 다양한 RL 알고리즘과 보상 함수를 쉽게 테스트하고 평가할 수 있는 표준화된 프레임워크를 구축한다.

### 1.3. 핵심 동기: 왜 강화학습인가?
기존의 상태 머신(FSM)이나 휴리스틱 기반 AI는 개발자가 모든 상황을 예측하여 규칙을 직접 코딩해야 하므로, 복잡한 상황에 대한 대처가 미흡하고 패턴이 고정되어 있다. 반면, 강화학습(RL)은 AI가 수많은 플레이 경험을 통해 스스로 최적의 전략을 학습하므로, 다음과 같은 장점을 가진다.

-   **확장성**: 새로운 캐릭터나 기술이 추가되어도, 환경(Environment)만 수정하면 AI가 스스로 새로운 전략을 학습할 수 있다.
-   **자율 학습**: 개발자의 개입을 최소화하면서 24시간 내내 스스로 강해지는 모델을 만들 수 있다.
-   **복잡성 대응**: 인간이 예측하기 어려운 복잡한 상황에서도 유연하고 창의적인 해결책을 찾아낼 수 있다.

---

## 2. 시스템 아키텍처

WebRTC 데이터 채널을 통해 프론트엔드 게임과 백엔드 RL 에이전트가 통신하는 P2P 구조이다.

```mermaid
graph TD
    subgraph Frontend (Browser)
        A[Arcade Clash Game]
        B[WebRTC Signaling Client]
    end

    subgraph Backend (Python)
        C[FightingEnv (Gym)]
        D[RL Agent (PPO)]
    end

    A -- Observation/Reward --> C
    C -- Action --> A
    C <--> D

    B <--> SignalingServer
```
*Note: 위 다이어그램은 데이터 흐름을 나타내며, 실제 통신은 WebRTC 데이터 채널을 통해 이루어짐.*

---

## 3. 주요 유즈케이스 (Use Cases)

-   **액터**: 개발자 (Developer), 시스템 (System)

### UC-01: AI 모델 성공적인 학습
-   **목표**: AI 에이전트가 게임을 플레이하며 학습하고, 그 결과로 학습된 모델 파일과 로그가 생성된다.
-   **주요 흐름**:
    1.  **개발자**는 백엔드 학습 스크립트(`train_rl_agent.py`)를 실행한다.
    2.  **시스템(백엔드)**은 자신의 Peer ID를 생성하여 시그널링 서버에 등록하고, 프론트엔드의 연결을 대기한다.
    3.  **개발자**는 브라우저에서 백엔드 Peer ID를 포함한 URL로 접속한다. (예: `...?mode=rl_training&backend_peer_id=...`)
    4.  **시스템(프론트엔드)**은 백엔드 Peer ID로 WebRTC 연결을 시도한다.
    5.  연결이 수립되면, **시스템(프론트엔드)**은 `connection_ready` 메시지를 백엔드로 전송한다.
    6.  **시스템(백엔드)**은 `connection_ready`를 받고, `reset` 명령을 보내며 학습 루프를 시작한다.
    7.  학습이 완료되면, **시스템(백엔드)**은 모델 파일(`*.zip`)과 로그를 지정된 경로에 저장하고 종료한다.

### UC-02: WebRTC 연결 실패
-   **목표**: 잘못된 정보로 인해 WebRTC 연결이 실패했을 때, 시스템이 오류를 명확히 인지시키고 비정상 종료되지 않는다.
-   **주요 흐름**:
    1.  **개발자**가 브라우저 URL에 **잘못된** 백엔드 Peer ID를 입력한다.
    2.  **시스템(프론트엔드)**은 연결에 실패하고, 화면에 "백엔드 연결 실패" 오류 메시지를 표시한다.
    3.  **시스템(백엔드)**은 설정된 시간(timeout) 동안 연결이 들어오지 않으면, "Frontend connection timed out." 로그를 남기고 정상적으로 프로세스를 종료한다.

### UC-03: 학습 중 연결 끊김
-   **목표**: 학습 도중 예기치 않게 연결이 끊겼을 때, 시스템이 현재까지의 학습 진행 상황을 최대한 보존하고 안전하게 종료된다.
-   **주요 흐름**:
    1.  학습 루프가 실행되던 중, 브라우저 탭이 닫히는 등 연결이 끊긴다.
    2.  **시스템(백엔드)**의 `FightingEnv.step()` 메서드가 프론트엔드로부터의 응답을 기다리다 타임아웃이 발생한다.
    3.  **시스템(백엔드)**은 예외를 감지하고, `model.save()`를 호출하여 현재까지 학습된 모델을 중간 저장한다.
    4.  "Connection lost during training. Saving current model..." 로그를 남기고 프로세스를 종료한다.

### UC-04: 학습 결과 확인
-   **목표**: 완료된 학습의 성과를 시각적으로 확인한다.
-   **주요 흐름**:
    1.  **개발자**는 `tensorboard --logdir ./logs` 명령을 실행한다.
    2.  **시스템(TensorBoard)**은 로그 파일을 읽어 웹 서버를 실행한다.
    3.  **개발자**는 브라우저에서 해당 URL로 접속하여 평균 보상 등의 학습 지표를 그래프로 확인한다.

---

## 4. 향후 계획

-   **멀티 에이전트 강화학습 (Multi-Agent RL)**: 두 AI 에이전트가 서로를 상대로 학습하게 하여 경쟁적으로 전략을 발전시킨다.
-   **모방 학습 (Imitation Learning)**: 숙련된 인간 플레이어의 리플레이 데이터를 사용하여 초기 모델이 인간과 유사한 행동을 빠르게 학습하도록 한다.
