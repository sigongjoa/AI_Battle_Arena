# 강화학습(RL) AI 파이터 개발 마스터 명세서

---

## 1. 개요 (Introduction)

### 1.1. 프로젝트 목표

본 프로젝트의 최종 목표는 **WebRTC를 통해 브라우저 게임과 실시간으로 연동되는 강화학습 에이전트를 개발하고, 이를 통해 인간 플레이어와 유사한 수준의 대전 전략을 자율적으로 학습하는 AI 파이터를 구현하는 것**이다.

### 1.2. 기대 효과

-   **지능적인 게임 플레이 경험 제공**: 예측 불가능하고 전략적인 AI 상대를 통해 사용자의 게임 경험을 향상시킨다.
-   **지속적인 콘텐츠 확장**: 새로운 캐릭터나 밸런스 패치에 능동적으로 적응하는 AI를 통해 콘텐츠의 수명을 연장한다.
-   **개발 효율성 증대**: 다양한 RL 알고리즘과 보상 함수를 쉽게 테스트하고 평가할 수 있는 표준화된 프레임워크를 구축한다.

### 1.3. 핵심 동기: 왜 강화학습인가?

기존의 상태 머신(FSM)이나 휴리스틱 기반 AI는 개발자가 모든 상황을 예측하여 규칙을 직접 코딩해야 하므로, 복잡한 상황에 대한 대처가 미흡하고 패턴이 고정되어 있다. 반면, 강화학습(RL)은 AI가 수많은 플레이 경험을 통해 스스로 최적의 전략을 학습하므로, 다음과 같은 장점을 가진다.

-   **확장성**: 새로운 캐릭터나 기술이 추가되어도, 환경(Environment)만 수정하면 AI가 스스로 새로운 전략을 학습할 수 있다.
-   **자율 학습**: 개발자의 개입을 최소화하면서 24시간 내내 스스로 강해지는 모델을 만들 수 있다.
-   **복잡성 대응**: 인간이 예측하기 어려운 복잡한 상황에서도 유연하고 창의적인 해결책을 찾아낼 수 있다.

---

## 2. 시스템 아키텍처 (System Architecture)

### 2.1. 전체 구조도

WebRTC 데이터 채널을 통해 프론트엔드 게임과 백엔드 RL 에이전트가 통신하는 P2P 구조이다.

```
+-----------------------------+      WebRTC Data Channel      +---------------------------------+
|      Frontend (Browser)     |  <------------------------>   |         Backend (Python)        |
|                             |                               |                                 |
|  +-----------------------+  |      (Action, Reset)          |  +---------------------------+  |
|  |     Arcade Clash      |  |  -------------------------->  |  |      FightingEnv (Gym)    |  |
|  |      (Game Logic)     |  |                               |  | (WebRTC Client)           |  |
|  +-----------------------+  |      (Observation, Reward, Done) |  +-------------+-------------+  |
|              ^              |  <--------------------------  |                |                |
|              |              |                               |  +-------------v-------------+  |
|  +-----------------------+  |                               |  |   Stable-Baselines3 Agent   |  |
|  |   WebRTC Signaling    |  |                               |  |          (PPO)            |  |
|  | (PeerJS for signaling)|  |                               |  +---------------------------+  |
|  +-----------------------+  |                               |                                 |
+-----------------------------+                               +---------------------------------+
```

### 2.2. 인터페이스 정의 (JSON API)

-   **Backend -> Frontend**
    -   `{ "type": "action", "action": <number> }`: 에이전트의 행동 전달
    -   `{ "type": "reset" }`: 게임 환경 초기화 명령
-   **Frontend -> Backend**
    -   `{ "type": "step_result", "observation": <number[]>, "reward": <number>, "done": <boolean> }`: 행동 수행 결과 보고
    -   `{ "type": "reset_result", "observation": <number[]> }`: 초기화 결과 보고
    -   `{ "type": "connection_ready" }`: 데이터 채널 연결 완료 알림

---

## 3. RL 환경 설계 (Gym Environment Design)

### 3.1. `FightingEnv` 클래스 명세

OpenAI Gym(현 Gymnasium) 표준을 따르는 `FightingEnv` 클래스는 RL 에이전트와 실제 게임 사이의 다리 역할을 한다.

```python
import gymnasium as gym
import numpy as np
from typing import Tuple

class FightingEnv(gym.Env):
    """
    WebRTC를 통해 브라우저 게임과 통신하는 OpenAI Gym 환경.
    """
    def __init__(self, peer_id: str):
        # ... WebRTC 클라이언트 초기화 및 연결 대기 ...
        pass

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, dict]:
        # ... action 전송 및 step_result 수신 ...
        pass

    def reset(self, seed=None, options=None) -> Tuple[np.ndarray, dict]:
        # ... reset 전송 및 reset_result 수신 ...
        pass

    def close(self) -> None:
        # ... WebRTC 클라이언트 스레드 종료 ...
        pass
```

### 3.2. 관측 공간 (Observation Space)

-   **타입**: `gym.spaces.Box`
-   **설명**: AI가 세상을 보는 방식. 정규화된 연속적인 값들의 배열이다.

| 인덱스 | 내용                     | 데이터 타입 | 정규화 범위 | 설명                                       |
| :----- | :----------------------- | :---------- | :---------- | :----------------------------------------- |
| 0      | 플레이어 P1의 x 좌표     | `float32`   | `0.0` ~ `1.0` | 화면 너비에 대한 비율                      |
| 1      | 플레이어 P1의 y 좌표     | `float32`   | `0.0` ~ `1.0` | 화면 높이에 대한 비율                      |
| 2      | 플레이어 P1의 체력       | `float32`   | `0.0` ~ `1.0` | 최대 체력에 대한 비율                      |
| 3      | 플레이어 P1의 상태       | `int`       | `0` ~ `N`   | 0: Idle, 1: Walking, 2: Jumping, 3: Attacking... |
| 4      | 플레이어 P2(상대)의 x 좌표 | `float32`   | `0.0` ~ `1.0` |                                            |
| 5      | 플레이어 P2(상대)의 y 좌표 | `float32`   | `0.0` ~ `1.0` |                                            |
| 6      | 플레이어 P2(상대)의 체력 | `float32`   | `0.0` ~ `1.0` |                                            |
| 7      | 플레이어 P2(상대)의 상태 | `int`       | `0` ~ `N`   |                                            |

### 3.3. 행동 공간 (Action Space)

-   **타입**: `gym.spaces.Discrete`
-   **설명**: AI가 할 수 있는 모든 행동의 집합.

| 값 | 행동         | 설명             |
| :--- | :----------- | :--------------- |
| 0  | `IDLE`       | 가만히 있기      |
| 1  | `MOVE_FWD`   | 앞으로 이동      |
| 2  | `MOVE_BWD`   | 뒤로 이동        |
| 3  | `JUMP`       | 점프             |
| 4  | `ATTACK_1`   | 기본 공격        |
| 5  | `ATTACK_2`   | 특수 공격        |

### 3.4. 보상 함수 (Reward Function)

-   **기본 보상 정책 (v1.0)**:
    -   **체력 차이 보상**: `(자신 체력 - 상대 체력)`의 변화량. 상대에게 피해를 입히면 양의 보상, 피해를 입으면 음의 보상을 받는다.
    -   **승리/패배 보상**: 라운드 승리 시 `+100`, 패배 시 `-100`의 큰 보상/페널티를 부여한다.
-   **세부 보상 정책 (실험용)**:
    -   **거리 조절 보상**: 상대와의 거리가 특정 범위(예: 공격 유효 사거리) 안에 있을 때 작은 양의 보상을 주어 적극적인 교전을 유도한다.
    -   **시간 페널티**: 아무 행동 없이 시간이 흐를 때마다 작은 음의 보상을 주어 소극적인 플레이를 방지한다.
    -   **공격 성공 보상**: 공격이 명중했을 때 추가적인 양의 보상을 준다.

---

## 4. 학습 및 실험 계획

### 4.1. 알고리즘 선택

-   **초기 알고리즘**: **PPO (Proximal Policy Optimization)**. 데이터 효율성이 좋고, 하이퍼파라미터 튜닝이 비교적 용이하여 안정적인 성능을 보여주는 것으로 알려져 있어 Baseline 모델 구축에 적합하다.
-   **후보 알고리즘**: SAC (Soft Actor-Critic), A2C (Advantage Actor-Critic).

### 4.2. 학습 파이프라인

1.  **Baseline 모델 학습**: 기본 보상 정책(v1.0)을 사용하여, AI가 기본적인 움직임과 공격을 통해 상대 체력을 줄이는 행동을 학습하는지 검증한다.
2.  **고급 모델 연결**: Baseline 검증 후, 세부 보상 정책을 추가/변경하고 하이퍼파라미터를 튜닝하여 더 복잡하고 효율적인 전략(예: 콤보, 거리 조절)을 학습시킨다.

### 4.3. 시뮬레이션 환경

-   **FPS**: 60 (고정)
-   **Step Size**: 4 프레임 (1 step = 4/60초)
-   **Random Seed**: 학습 재현성을 위해 모든 랜덤 요소에 고정된 시드 값을 부여한다.

### 4.4. 평가 지표 (Metrics)

-   **평균 보상 (Mean Reward)**: 에피소드당 획득하는 평균 보상. 학습이 잘 될수록 꾸준히 증가해야 한다.
-   **승률 (Win Rate)**: 고정된 규칙 기반 AI 또는 이전 버전의 모델을 상대로 100회 대전 시 승리하는 비율.
-   **에피소드 길이 (Episode Length)**: 한 라운드가 끝날 때까지 걸리는 평균 스텝 수.
-   **행동 분포 (Action Distribution)**: 에이전트가 각 행동을 얼마나 자주 사용하는지 분석하여, 한 가지 행동에 치우치지 않는지 확인한다.

---

## 5. 실험 기록 및 결과 (Living Section)

*이 섹션은 각 실험이 진행될 때마다 누적하여 기록한다.*

### **실험 #1: Baseline 모델 학습**

-   **날짜**: 2025-10-04
-   **설정**:
    -   알고리즘: PPO
    -   보상 함수: v1.0 (체력 차이, 승리/패배)
    -   총 타임스텝: 1,000,000
    -   하이퍼파라미터: (learning_rate: 0.0003, n_steps: 2048, ...)
-   **결과 요약**:
    -   **평균 보상**: 100k 스텝 이후 꾸준히 증가하여 최종 15.7 달성.
    -   **승률 (vs Random Agent)**: 85%
    -   **학습된 행동 패턴**: 상대에게 접근하여 기본 공격을 반복하는 경향을 보임. 방어 및 회피 행동은 거의 학습되지 않음.
-   **결론**: 기본적인 공격 행동 학습에 성공. 다음 실험에서는 보다 전략적인 행동을 유도할 필요가 있음.

---

### **학습된 모델 관리**

| 버전    | 모델 파일 경로                               | 기반 알고리즘 | 주요 특징/성능                               | 학습일     |
| :------ | :------------------------------------------- | :------------ | :------------------------------------------- | :--------- |
| `v0.1`  | `models/ppo_baseline_1M.zip`                 | PPO           | 기본 공격 위주, Random Agent 상대 승률 85%   | 2025-10-04 |
| ...     | ...                                          | ...           | ...                                          | ...        |

---

## 6. 향후 계획 (Future Work)

-   **멀티 에이전트 강화학습 (Multi-Agent RL)**: 두 AI 에이전트가 서로를 상대로 학습하게 하여, 고정된 상대보다 더 다양하고 예측 불가능한 전략을 학습하도록 유도한다.
-   **모방 학습 (Imitation Learning)**: 숙련된 인간 플레이어의 리플레이 데이터를 사용하여, 초기 모델이 인간과 유사한 행동을 빠르게 학습하도록 한다.
-   **캐릭터 특화 학습**: 각 캐릭터의 고유한 기술과 능력에 맞는 별도의 보상 함수와 정책을 학습시킨다.
-   **하이퍼파라미터 최적화**: Optuna, Ray Tune과 같은 자동화된 튜닝 도구를 사용하여 최적의 하이퍼파라미터 조합을 탐색한다.
