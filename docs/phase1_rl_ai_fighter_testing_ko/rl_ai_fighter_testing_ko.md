# 강화학습(RL) 기반 AI 파이터 테스트 명세서

## 1. 개요 (Introduction)

본 문서는 강화학습(RL) 기반 AI 파이터 시스템의 개발 및 통합 과정에서 필요한 테스트 환경 설정, 테스트 진행 방향, 그리고 성공 판단 기준을 정의합니다. 이는 `FightingEnv` 환경의 정확성 검증, RL 에이전트 학습의 안정성 확인, 그리고 학습된 AI의 성능 평가를 목적으로 합니다.

## 2. 테스트 환경 설정 (Test Environment Setup)

RL AI 파이터의 개발 및 테스트를 위한 환경 설정은 다음과 같습니다.

*   **필요 라이브러리:**
    *   `gym` (OpenAI Gym 환경 인터페이스)
    *   `stable-baselines3` (RL 알고리즘 구현체)
    *   `pytorch` (딥러닝 프레임워크, `stable-baselines3`의 백엔드)
    *   `numpy` (수치 계산)
    *   `pygame` (게임 엔진)
    *   `pytest` (테스트 프레임워크)

*   **가상 환경 설정:**
    *   Python 프로젝트의 의존성 관리를 위해 `venv` 또는 `conda`와 같은 가상 환경 사용을 권장합니다.
    *   `pip install -r requirements.txt` 또는 `conda install --file requirements.txt`를 통해 모든 의존성을 설치합니다.

*   **하드웨어 요구사항:**
    *   RL 학습은 연산량이 많으므로, GPU(NVIDIA CUDA 지원) 사용을 권장합니다. `pytorch` 설치 시 CUDA 버전에 맞는 버전을 설치해야 합니다.
    *   CPU만으로도 학습은 가능하나, 학습 시간이 매우 길어질 수 있습니다.

## 3. 테스트 진행 방향 (Testing Directions)

RL AI 파이터의 테스트는 유닛 테스트, 통합 테스트, 성능 테스트로 구분하여 진행합니다.

### 3.1. 유닛 테스트 (Unit Tests)

각 모듈 및 함수의 개별적인 기능과 정확성을 검증합니다.

*   **`FightingEnv` 클래스:**
    *   `__init__` 메서드: `observation_space`와 `action_space`가 설계 문서에 따라 올바르게 정의되었는지 확인합니다.
    *   `reset()` 메서드: 환경이 초기 상태로 정확히 리셋되고, 반환되는 초기 관측(Observation)이 유효한지 검증합니다.
    *   `step(action)` 메서드:
        *   주어진 `action`이 게임에 올바르게 적용되는지 확인합니다.
        *   반환되는 `next_state`가 `observation_space`의 범위 내에 있으며, 게임 상태를 정확히 반영하는지 검증합니다.
        *   `reward` 계산 로직이 설계된 보상 함수에 따라 정확하게 동작하는지 확인합니다.
        *   `done` 플래그가 게임 종료 조건(예: HP 0)에서 올바르게 `True`로 설정되는지 검증합니다.
*   **보상 함수 로직:** `FightingEnv.step()` 내에서 보상 함수가 각 게임 이벤트(데미지, KO, 거리 변화 등)에 대해 예상된 보상 값을 반환하는지 시나리오별로 테스트합니다.
*   **`get_game_state` 및 `apply_ai_action` 인터페이스:** 이 함수들이 게임 엔진과 RL 환경 간의 데이터 교환 및 행동 적용을 정확하게 수행하는지 별도의 테스트 케이스로 검증합니다.

### 3.2. 통합 테스트 (Integration Tests)

여러 모듈이 함께 동작할 때의 상호작용과 시스템의 안정성을 검증합니다.

*   **`FightingEnv`와 RL 알고리즘 연동:**
    *   `Stable-Baselines3`의 PPO 또는 A2C 알고리즘을 `FightingEnv`에 연결하여 학습을 시작하고, 에러 없이 정상적으로 학습이 진행되는지 확인합니다.
    *   학습 과정에서 `env.step()` 및 `env.reset()` 호출이 안정적으로 이루어지는지 모니터링합니다.
*   **학습 과정의 안정성:**
    *   최소 10,000 스텝 이상 에러 없이 학습이 진행되는지 확인합니다.
    *   학습 로그(예: `tensorboard` 또는 `verbose` 출력)를 통해 `episode_reward` 및 `loss` 값의 변화를 모니터링하여 학습이 수렴하는 경향을 보이는지 확인합니다.
*   **학습된 모델의 게임 플레이 연동:**
    *   학습이 완료된 RL 모델을 `AIController`에 통합하여 실제 Pygame 프로토타입에서 플레이어 2를 제어하도록 설정합니다.
    *   게임 플레이 중 AI가 크래시 없이 안정적으로 동작하는지 확인합니다.

### 3.3. 성능 테스트 (Performance Tests)

시스템의 효율성과 AI의 학습 및 추론 속도를 평가합니다.

*   **학습 시간 측정:** 특정 `total_timesteps` (예: 100,000 스텝)를 학습하는 데 걸리는 시간을 측정합니다. (CPU/GPU 환경별 비교)
*   **학습된 AI의 게임 내 FPS 영향:** 학습된 AI가 게임 플레이에 통합되었을 때, 게임의 프레임 속도(FPS)에 부정적인 영향을 미 미치는지 확인합니다.

## 4. 성공 판단 기준 (Success Criteria)

RL 기반 AI 파이터 시스템의 성공은 다음 기준을 충족할 때 판단합니다.

### 4.1. 환경 구현
*   `FightingEnv` 클래스가 OpenAI Gym API 표준을 완벽하게 준수하며, `reset()`, `step()`, `render()`, `close()` 메서드가 예상대로 동작합니다.
*   `observation_space`와 `action_space`가 설계 문서에 명시된 범위와 타입에 맞게 정확히 정의됩니다.

### 4.2. 학습 안정성
*   RL 에이전트가 최소 100,000 스텝 이상 에러 없이 학습을 완료합니다.
*   학습 과정에서 `episode_reward`가 지속적으로 증가하거나 안정적인 수렴 경향을 보입니다.

### 4.3. AI 성능 (정량적)
*   **승률:** 학습된 AI가 무작위 행동을 하는 AI 또는 기존 FSM AI(현재 Pygame 프로토타입의 AI)를 상대로 50% 이상의 승률을 달성합니다.
*   **보상 함수 값:** 학습 완료 후, 에이전트가 게임을 플레이할 때 얻는 평균 보상 값이 학습 초기 대비 유의미하게 증가합니다.

### 4.4. AI 성능 (정성적)
*   학습된 AI가 게임 내에서 단순히 무작위 행동을 하는 것을 넘어, 합리적이고 다양한 격투 전략(예: 공격, 방어, 회피, 거리 조절)을 구사하는 것을 육안으로 확인합니다.
*   특정 상황(예: 체력 열세 시 방어, 상대방 공격 시 회피 또는 반격)에 대한 적절한 반응을 보입니다.

### 4.5. 시스템 안정성
*   RL 학습 과정 및 학습된 AI를 이용한 게임 플레이 중 치명적인 오류(크래시) 또는 비정상 종료가 발생하지 않습니다.
*   학습된 AI가 게임에 통합된 후에도 게임의 프레임 속도가 허용 가능한 수준(예: 30 FPS 이상)으로 유지됩니다.
