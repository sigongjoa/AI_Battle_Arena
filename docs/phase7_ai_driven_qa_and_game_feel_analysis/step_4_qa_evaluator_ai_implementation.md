# Phase 7: Step 4 - QA Evaluator AI 구현 (Multi-Persona Analysis & Evaluation)

## 1. 목표
다양한 AI 페르소나를 학습시키고, 이들을 활용하여 게임의 QA 메트릭을 다각적으로 평가하며, 인간 피드백 기반 강화학습(RLHF) 프레임워크를 구축합니다.

## 2. 상세 계획

### 2.1. AI 페르소나 정의 및 학습 환경 구축
*   **각 페르소나별 보상 함수 및 학습 파라미터 정의:** `초보자 AI`, `프로게이머 AI`, `압박형 AI`, `트롤 AI` 등 Phase 7 문서에 정의된 각 페르소나의 특성을 반영한 보상 함수 및 학습 파라미터 설정.
*   **RL 에이전트 학습 파이프라인 구축:** `Stable-Baselines3`를 사용하여 각 페르소나별 RL 에이전트 학습 파이프라인 구축.
*   **탐색 능력 강화:** `Curiosity-Driven Exploration` (ICM) 모듈을 선택적으로 적용하여 에이전트의 새로운 상태 탐색 능력 강화.
*   **`Penalty` (금지된 지식) 보상 로직 구현:** 특정 행동이나 상태에 진입 시 음수 보상을 부여하는 로직 구현.

### 2.2. 다중 페르소나 분석 로직 구현
*   **시뮬레이션 실행:** 각 페르소나 AI가 동일한 게임 시나리오를 플레이하도록 시뮬레이션 실행 및 로그 수집.
*   **교차 분석:** 각 페르소나의 플레이 결과(메트릭)를 비교 분석하여, 특정 문제(예: 밸런스 불균형)가 어떤 플레이어 유형에게 더 큰 영향을 미치는지 교차 검증하는 로직 구현.
*   **모델 관리:** `TensorFlow/PyTorch`를 사용하여 다중 페르소나 모델 관리 및 평가 로직 구현.

### 2.3. RLHF (Human Feedback) 프레임워크 구축
*   **쌍대 비교(Pairwise Comparison) 방식 구현:**
    *   AI가 생성한 다양한 타격 시퀀스(또는 리플레이 클립)를 두 개씩 묶어 평가자에게 제시하는 웹/GUI 인터페이스 개발.
    *   평가자가 "어느 쪽의 손맛이 더 나은가?" (A, B, 비슷함)를 선택하도록 구현.
*   **보상 모델 학습:** 수집된 선호도 데이터를 기반으로 `Bradley-Terry` 모델 등을 사용하여 '손맛 점수'를 예측하는 보상 모델 (`R_θ(sequence)`) 학습.
*   **RL 에이전트 학습 활용:** 학습된 보상 모델을 RL 에이전트 학습에 활용하여 '손맛'을 최대화하는 행동 패턴 학습.

## 3. Definition of Done (DoD)
*   `초보자 AI`, `프로게이머 AI`, `압박형 AI`, `트롤 AI` 등 모든 정의된 AI 페르소나에 대한 보상 함수 및 학습 파라미터가 설정되었음.
*   `Stable-Baselines3`를 활용한 각 페르소나별 RL 에이전트 학습 파이프라인이 구축되었고, 기본적인 학습이 완료되었음.
*   `Curiosity-Driven Exploration` 및 `Penalty` 보상 로직이 구현되었고, 에이전트 학습에 적용되었음.
*   다중 페르소나 AI가 동일 시나리오를 플레이하고 그 결과를 비교 분석하는 로직이 구현되었음.
*   RLHF를 위한 쌍대 비교 웹/GUI 인터페이스가 개발되었고, 인간 평가 데이터를 수집할 수 있음.
*   수집된 RLHF 데이터를 기반으로 '손맛 점수'를 예측하는 보상 모델이 학습되었고, RL 에이전트 학습에 활용될 수 있음을 확인했음.