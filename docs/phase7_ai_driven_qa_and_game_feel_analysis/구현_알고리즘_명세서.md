# Phase 7: 구현 알고리즘 명세서 (v2.0)

## 1. RL 테스트 에이전트 (고도화)

### 1.1. 알고리즘 선정
- **PPO (Proximal Policy Optimization):** `Stable-Baselines3` 라이브러리를 통해 구현.
- **(추가) Curiosity-Driven Exploration:** `Stable-Baselines3-Contrib`의 `ICM` (Intrinsic Curiosity Module)을 선택적으로 적용하여 에이전트가 새로운 상태를 탐색하도록 장려한다.

### 1.2. 관찰 공간 (Observation Space) 및 행동 공간 (Action Space)
- (v1.0과 동일. 필요시 확장)

### 1.3. 통합 보상 함수 (v2.0)
- **`Reward = w_s*Stability + w_b*Balance + w_r*Responsiveness + w_i*Immersion + w_c*Curiosity - w_p*Penalty`**
  - **(추가) `Curiosity`:** ICM 모듈이 계산하는 내재적 보상(Intrinsic Reward). 에이전트가 예측하기 어려운 새로운 상태에 도달할수록 높은 값을 가진다.
  - **(추가) `Penalty`:** '금지된 지식(Taboo Knowledge)'에 대한 페널티. 특정 행동(예: 동일 행동 10프레임 이상 반복)이나 상태(예: 맵 구석에서 벗어나지 않음)에 진입 시 음수 보상을 부여한다.

### (추가) 1.4. AI 페르소나 구현 방안
- **'초보자 AI':**
  - **Action Masking:** 복잡한 콤보에 해당하는 액션 시퀀스를 마스킹하여 의도적으로 사용하지 못하게 한다.
  - **Human Error Layer:** 실수 주입 확률을 높게 설정한다.
- **'프로게이머 AI':**
  - **Reward Hacking:** 보상 함수에서 `Balance`, `Immersion` 등의 가중치를 낮추고, 승패와 관련된 `Damage` 보상을 극대화하여 학습시킨다.
  - **Human Error Layer:** 실수 주입 확률을 0에 가깝게 설정한다.
- **'압박형 AI':**
  - **Custom Reward:** 상대방과의 거리가 가까울수록, 그리고 공격 행동을 할수록 추가 보상을 받도록 커스텀 보상 함수를 설계하여 학습시킨다.

## 2. '손맛' (Immersion) 및 '리듬' 점수화 알고리즘 (v2.0)

### 2.1. '리듬' 기반 지표 추가
- 기존의 '순간' 기반 대리 변수 모델을 확장하여, 액션의 시계열 데이터를 분석하는 '리듬' 지표를 추가한다.
- **`TensionIndex` (긴장감 지수):** 공격이 명중하기 직전까지, 두 플레이어가 서로 공격을 시도하지 않고 탐색전을 벌인 시간으로 계산.
- **`ComboRhythmScore` (콤보 리듬 점수):** 콤보 내 각 타격 사이의 시간 간격의 표준편차. 일정한 리듬으로 이어지는 콤보일수록 높은 점수를 받는다.
- **`CounterPlayScore` (카운터플레이 점수):** 상대의 공격 종료 시점과 내 반격 시작 시점 사이의 시간차. 시간이 짧을수록(아슬아슬할수록) 높은 점수를 받는다.

### 2.2. RLHF 방식 고도화 (A/B 테스트)
- **(변경)** 인간 평가는 점수제가 아닌 **쌍대 비교(Pairwise Comparison)** 방식을 사용한다.
  1.  두 개의 다른 플레이 클립 (A, B)을 평가자에게 제시.
  2.  평가자는 "어느 쪽의 손맛이 더 나은가?" 라는 질문에 A, B, 또는 '비슷함'으로 답변.
  3.  수집된 선호도 데이터를 기반으로 `Bradley-Terry` 모델 등을 사용하여 보상 모델 `R_θ(sequence)`를 학습시킨다.

## (추가) 3. 시뮬레이션 환경 고도화 알고리즘

### 3.1. 인간적 실수 레이어 (Human Error Layer)
- **알고리즘:** AI 에이전트의 `Action` 객체를 입력받아, 아래 로직을 순차적으로 적용 후 게임 엔진으로 전달한다.
  1.  **입력 지연 (Input Lag):** `config.yaml`에 정의된 `reaction_time_mean`, `reaction_time_std`에 따라 정규분포에서 샘플링된 프레임 수만큼 액션 전달을 지연시킨다.
  2.  **입력 오타 (Input Mistake):** `mistake_probability` 확률로, `Action`의 구성 요소(예: 방향)를 다른 유효한 값으로 무작위 변경한다.
  3.  **입력 누락 (Input Drop):** `drop_probability` 확률로, 해당 프레임의 `Action`을 `None`으로 변경(입력 씹힘)한다.

### 3.2. 트롤 AI (Troll AI) 구현
- **목표:** 승리가 아닌, 상대방의 플레이를 방해하고 특정 패턴을 유도하는 것을 목표로 하는 특수 목적 에이전트.
- **보상 함수 설계:**
  - 상대방이 동일한 방어 행동을 계속하도록 유도하면 보상.
  - 상대방과 멀리 떨어져 시간을 지연시키면 보상.
  - 승리/패배에 대한 보상은 0으로 설정.
- **활용:** 일반 에이전트와 '트롤 AI'를 대전시켜, 특정 플레이 스타일에 대한 스트레스 유발 정도나 시스템의 취약점을 분석한다.

## 4. 밸런스 및 안정성 분석 알고리즘 (v2.0)

### 4.1. 밸런스 분석
- **(확장)** 승률 편향 및 콤보 효율성 분석을 **각 'AI 페르소나'별로 개별적으로 수행하고, 그 결과를 교차 분석**하여 리포트한다.

### 4.2. 안정성 및 반응성 분석
- (v1.0과 동일)
